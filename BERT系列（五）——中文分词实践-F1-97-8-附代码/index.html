<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>BERT系列（五）——中文分词实践 F1 97.8%(附代码) | 西溪雷神</title>
  <meta name="author" content="Jiangping Lei">
  
  <meta name="description" content="一、前言BERT源码解读完了，具体怎么用于自己的项目呢？在BERT系列（四）——源码解读之Fine-tune中，我说只要修改两个地方。
重要的是明白根据不同任务调整输入格式和对loss的构建，这两个知识点学会之后，基本上也可以依葫芦画瓢做一些自己的任务了。


那么这一次，我们就来依葫芦画瓢，作一个中文分词项目。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="BERT系列（五）——中文分词实践 F1 97.8%(附代码)"/>
  <meta property="og:site_name" content="西溪雷神"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="西溪雷神" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">西溪雷神</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article id="post-BERT系列（五）——中文分词实践-F1-97-8-附代码" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2019-01-11T08:59:00.000Z"><a href="/BERT系列（五）——中文分词实践-F1-97-8-附代码/">2019-01-11</a></time>
      
      
  
    <h1 class="p-name title" itemprop="headline name">BERT系列（五）——中文分词实践 F1 97.8%(附代码)</h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>BERT源码解读完了，具体怎么用于自己的项目呢？在<a href="../BERT系列（四）——源码解读之Fine-tune">BERT系列（四）——源码解读之Fine-tune</a>中，我说只要修改两个地方。</p>
<blockquote><p>重要的是明白根据不同任务调整输入格式和对loss的构建，这两个知识点学会之后，基本上也可以依葫芦画瓢做一些自己的任务了。</p>
</blockquote>

<p>那么这一次，我们就来依葫芦画瓢，作一个中文分词项目。</p>
<a id="more"></a>

<h1 id="二、数据准备"><a href="#二、数据准备" class="headerlink" title="二、数据准备"></a>二、数据准备</h1><p>数据是我经<a href="https://pan.baidu.com/s/1vVex2fHu5NRcSW9riLACFQ" target="_blank" rel="noopener">词性标注@人民日报199801.txt</a> 加工过后形成的2个样本文件，分别用于训练和验证，2份数据的格式一摸一样。</p>
<blockquote><p>test.txt<br>train.txt</p>
</blockquote>

<p>数据格式如下所示：</p>
<blockquote><p>黄土地上的蒲公英（外一首）      bmessbmesssss<br>同胞们、朋友们、女士们、先生们：        bessbessbessbess</p>
</blockquote>

<p>文字和标签用 \t 分隔开。相信做过分词的朋友都知道后面的标签是什么意思，不知道的我这里解释一下，分词是一个典型的基于字的标注任务，标签有四个分别是：b代表begin，m代表middle，e代表end，s代表single。</p>
<p>例如：<strong>黄土地上的蒲公英（外一首）</strong>按照自然切分呈现的是 <strong>黄土地 上 的 蒲公英 （ 外 一 首 ）</strong>。<strong>黄土地</strong>的标签是bme 说明黄是一个词的开头begin, 土是一个词的中间middle，地是一个词的结尾end，<strong>上</strong>、<strong>的</strong>是s说明这是一个单字词single，于是任意的字都可以有一个标签去对应。</p>
<h1 id="三、代码"><a href="#三、代码" class="headerlink" title="三、代码"></a>三、代码</h1><h2 id="1-转换输入格式"><a href="#1-转换输入格式" class="headerlink" title="1.转换输入格式"></a>1.转换输入格式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def get_labels(self):</span><br><span class="line">    return [&quot;s&quot;, &quot;b&quot;, &quot;m&quot;, &quot;e&quot;, &quot;X&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;]</span><br></pre></td></tr></table></figure>
<p>把Processor里的get_labels函数换成以下形式，其中”X”代表其他标签（虽然原则上每一个字符都能对应上面四种标签的任一个，但是因为WordpieceTokenizer的操作会分解个别字段，被分解的字符用”X”标记）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def _create_example(self, lines, set_type):</span><br><span class="line">    examples &#x3D; []</span><br><span class="line">    for (i, line) in enumerate(lines):</span><br><span class="line">        guid &#x3D; &quot;%s-%s&quot; % (set_type, i)</span><br><span class="line">        text &#x3D; tokenization.convert_to_unicode(line[0])</span><br><span class="line">        label &#x3D; tokenization.convert_to_unicode(line[1])</span><br><span class="line">        assert len(text) &#x3D;&#x3D; len(label)</span><br><span class="line">        examples.append(InputExample(guid&#x3D;guid, text&#x3D;text, label&#x3D;label))</span><br><span class="line">    return examples</span><br></pre></td></tr></table></figure>
<p>构造样本，没什么好说的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer,mode):</span><br><span class="line">    label_map &#x3D; &#123;&#125;</span><br><span class="line">    for (i, label) in enumerate(label_list,1):</span><br><span class="line">        label_map[label] &#x3D; i</span><br><span class="line">    label2idpath &#x3D; &#39;.&#x2F;output&#x2F;label2id.pkl&#39;</span><br><span class="line">    if not os.path.exists(label2idpath):</span><br><span class="line">        with open(label2idpath,&#39;wb&#39;) as w:</span><br><span class="line">            pickle.dump(label_map, w)</span><br><span class="line"></span><br><span class="line">    textlist &#x3D; list(example.text)</span><br><span class="line">    labellist &#x3D; list(example.label)</span><br><span class="line">    tokens &#x3D; []</span><br><span class="line">    labels &#x3D; []</span><br><span class="line">    unknow_index &#x3D; [] </span><br><span class="line">    for i, word in enumerate(textlist):</span><br><span class="line">        token &#x3D; tokenizer.tokenize(word)</span><br><span class="line">        tokens.extend(token)</span><br><span class="line">        label_1 &#x3D; labellist[i]</span><br><span class="line">        for m in range(len(token)):</span><br><span class="line">            if m &#x3D;&#x3D; 0:</span><br><span class="line">                labels.append(label_1)</span><br><span class="line">            else:</span><br><span class="line">                labels.append(&quot;X&quot;)</span><br><span class="line">            if token[m] &#x3D;&#x3D; &quot;[UNK]&quot;:</span><br><span class="line">                unknow_index.append(i)</span><br><span class="line">    assert len(tokens) &#x3D;&#x3D; len(labels)</span><br><span class="line">    if len(tokens) &gt;&#x3D; max_seq_length - 1:</span><br><span class="line">        tokens &#x3D; tokens[0:(max_seq_length - 2)]</span><br><span class="line">        labels &#x3D; labels[0:(max_seq_length - 2)]</span><br><span class="line">    ntokens &#x3D; []</span><br><span class="line">    segment_ids &#x3D; []</span><br><span class="line">    label_ids &#x3D; []</span><br><span class="line">    ntokens.append(&quot;[CLS]&quot;)</span><br><span class="line">    segment_ids.append(0)</span><br><span class="line">    label_ids.append(label_map[&quot;[CLS]&quot;])</span><br><span class="line">    for i, token in enumerate(tokens):</span><br><span class="line">        ntokens.append(token)</span><br><span class="line">        segment_ids.append(0)</span><br><span class="line">        label_ids.append(label_map[labels[i]])</span><br><span class="line">    ntokens.append(&quot;[SEP]&quot;)</span><br><span class="line">    segment_ids.append(0)</span><br><span class="line">    label_ids.append(label_map[&quot;[SEP]&quot;])</span><br><span class="line">    input_ids &#x3D; tokenizer.convert_tokens_to_ids(ntokens)</span><br><span class="line">    input_mask &#x3D; [1] * len(input_ids)</span><br><span class="line">    while len(input_ids) &lt; max_seq_length:</span><br><span class="line">        input_ids.append(0)</span><br><span class="line">        input_mask.append(0)</span><br><span class="line">        segment_ids.append(0)</span><br><span class="line">        label_ids.append(0)</span><br><span class="line">        ntokens.append(&quot;**NULL**&quot;)</span><br><span class="line">    assert len(input_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    assert len(input_mask) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    assert len(segment_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    assert len(label_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    if ex_index &lt; 5:</span><br><span class="line">        tf.logging.info(&quot;*** Example ***&quot;)</span><br><span class="line">        tf.logging.info(&quot;guid: %s&quot; % (example.guid))</span><br><span class="line">        tf.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in tokens]))</span><br><span class="line">        tf.logging.info(&quot;input_ids: %s&quot; % &quot; &quot;.join([str(x) for x in input_ids]))</span><br><span class="line">        tf.logging.info(&quot;input_mask: %s&quot; % &quot; &quot;.join([str(x) for x in input_mask]))</span><br><span class="line">        tf.logging.info(&quot;segment_ids: %s&quot; % &quot; &quot;.join([str(x) for x in segment_ids]))</span><br><span class="line">        tf.logging.info(&quot;label_ids: %s&quot; % &quot; &quot;.join([str(x) for x in label_ids]))</span><br><span class="line">    feature &#x3D; InputFeatures(</span><br><span class="line">        input_ids&#x3D;input_ids,</span><br><span class="line">        input_mask&#x3D;input_mask,</span><br><span class="line">        segment_ids&#x3D;segment_ids,</span><br><span class="line">        label_ids&#x3D;label_ids,</span><br><span class="line">    )</span><br><span class="line">    output_tokens &#x3D; []</span><br><span class="line">    for i, each in enumerate(ntokens):</span><br><span class="line">        if each !&#x3D; &quot;[UNK]&quot;:</span><br><span class="line">            output_tokens.append(each)</span><br><span class="line">        else:</span><br><span class="line">            index &#x3D; unknow_index[0]</span><br><span class="line">            output_tokens.append(textlist[index])</span><br><span class="line">            unknow_index &#x3D; unknow_index[1:]</span><br><span class="line">    write_tokens(output_tokens, mode)</span><br><span class="line">    return feature</span><br></pre></td></tr></table></figure>
<p>和run_squad.py以及run_classifier.py一样，将example转换成feature，每一个ntokens的格式：[CLS]句子[SEP]，超出指定长度的truncate，不足的用0补齐。</p>
<h2 id="2-构造loss"><a href="#2-构造loss" class="headerlink" title="2.构造loss"></a>2.构造loss</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def create_model(bert_config, is_training, input_ids, input_mask,</span><br><span class="line">                 segment_ids, labels, num_labels, use_one_hot_embeddings):</span><br><span class="line">    model &#x3D; modeling.BertModel(</span><br><span class="line">        config&#x3D;bert_config,</span><br><span class="line">        is_training&#x3D;is_training,</span><br><span class="line">        input_ids&#x3D;input_ids,</span><br><span class="line">        input_mask&#x3D;input_mask,</span><br><span class="line">        token_type_ids&#x3D;segment_ids,</span><br><span class="line">        use_one_hot_embeddings&#x3D;use_one_hot_embeddings</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    output_layer &#x3D; model.get_sequence_output()</span><br><span class="line"></span><br><span class="line">    hidden_size &#x3D; output_layer.shape[-1].value</span><br><span class="line"></span><br><span class="line">    output_weight &#x3D; tf.get_variable(</span><br><span class="line">        &quot;output_weights&quot;, [num_labels, hidden_size],</span><br><span class="line">        initializer&#x3D;tf.truncated_normal_initializer(stddev&#x3D;0.02)</span><br><span class="line">    )</span><br><span class="line">    output_bias &#x3D; tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;, [num_labels], initializer&#x3D;tf.zeros_initializer()</span><br><span class="line">    )</span><br><span class="line">    with tf.variable_scope(&quot;loss&quot;):</span><br><span class="line">        if is_training:</span><br><span class="line">            output_layer &#x3D; tf.nn.dropout(output_layer, keep_prob&#x3D;0.9)</span><br><span class="line">        output_layer &#x3D; tf.reshape(output_layer, [-1, hidden_size])</span><br><span class="line">        logits &#x3D; tf.matmul(output_layer, output_weight, transpose_b&#x3D;True)</span><br><span class="line">        logits &#x3D; tf.nn.bias_add(logits, output_bias)</span><br><span class="line">        logits &#x3D; tf.reshape(logits, [-1, FLAGS.max_seq_length, num_labels])</span><br><span class="line">        log_probs &#x3D; tf.nn.log_softmax(logits, axis&#x3D;-1)</span><br><span class="line">        one_hot_labels &#x3D; tf.one_hot(labels, depth&#x3D;num_labels, dtype&#x3D;tf.float32)</span><br><span class="line">        per_example_loss &#x3D; -tf.reduce_sum(one_hot_labels * log_probs, axis&#x3D;-1)</span><br><span class="line">        loss &#x3D; tf.reduce_sum(per_example_loss)</span><br><span class="line">        probabilities &#x3D; tf.nn.softmax(logits, axis&#x3D;-1)</span><br><span class="line">        predict &#x3D; tf.argmax(probabilities, axis&#x3D;-1)</span><br><span class="line">        return (loss, per_example_loss, logits, predict)</span><br></pre></td></tr></table></figure>
<p>取模型最后一层输出的sequence_output（shape [batch_size, seq_length, hidden_size]），然后线性投影到标签上(shape [batch_size, seq_length, num_labels]) ，这里num_labels=8，7个标签+补齐用的0；</p>
<p>再计算交叉熵损失loss 和 预测的标签predict</p>
<h1 id="四、运行-amp-amp-评估"><a href="#四、运行-amp-amp-评估" class="headerlink" title="四、运行&amp;&amp;评估"></a>四、运行&amp;&amp;评估</h1><p>训练集有39404个，GPU运行3个epochs，耗时大概在28分钟</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 run_cut.py   --task_name&#x3D;&quot;people&quot;   --do_train&#x3D;True   --do_predict&#x3D;True  --data_dir&#x3D;$PEOPLEcut    --vocab_file&#x3D;$BERT_CHINESE_DIR&#x2F;vocab.txt   --bert_config_file&#x3D;$BERT_CHINESE_DIR&#x2F;bert_config.json   --init_checkpoint&#x3D;$BERT_CHINESE_DIR&#x2F;bert_model.ckpt    --max_seq_length&#x3D;128    --train_batch_size&#x3D;32    --learning_rate&#x3D;2e-5   --num_train_epochs&#x3D;3.0    --output_dir&#x3D;.&#x2F;output&#x2F;result_cut&#x2F;</span><br></pre></td></tr></table></figure>
<p>评估方法见<a href="https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%E5%88%86%E8%AF%8D%E6%95%88%E6%9E%9C%E7%9A%84%E8%AF%84%E6%B5%8B%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">中文分词器分词效果的评测方法</a>，以下是采用测试集的评估结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INFO:tensorflow:***** Eval results *****</span><br><span class="line">INFO:tensorflow:  count &#x3D; 9925</span><br><span class="line">INFO:tensorflow:  precision_avg &#x3D; 0.9794</span><br><span class="line">INFO:tensorflow:  recall_avg &#x3D; 0.9780</span><br><span class="line">INFO:tensorflow:  f1_avg &#x3D; 0.9783</span><br><span class="line">INFO:tensorflow:  error_avg &#x3D; 0.0213</span><br></pre></td></tr></table></figure>
<p>测试数据用于测试效果，其中第一行为原文本，第二行为标签分词、第三行为预测分词：</p>
<blockquote><p>国民经济保持了“高增长、低通胀”的良好发展态势<br>国民经济 保持 了 “ 高 增长 、 低 通胀 ” 的 良好 发展 态势<br>国民经济 保持 了 “ 高 增长 、 低 通胀 ” 的 良好 发展 态势</p>
<p>农业生产再次获得好的收成，企业改革继续深化，人民生活进一步改善<br>农业 生产 再次 获得 好 的 收成 ， 企业 改革 继续 深化 ， 人民 生活 进一步 改善<br>农业 生产 再次 获得 好 的 收成 ， 企业 改革 继续 深化 ， 人民 生活 进一步 改善</p>
<p>这些外交活动，符合和平与发展的时代主题，顺应世界走向多极化的趋势，对于促进国际社会的友好合作和共同发展作出了积极的贡献<br>这些 外交 活动 ， 符合 和平 与 发展 的 时代 主题 ， 顺应 世界 走向 多极化 的 趋势 ， 对于 促进 国际 社会 的 友好 合作 和 共同 发展 作出 了 积极 的 贡献<br>这些 外交 活动 ， 符合 和平 与 发展 的 时代 主题 ， 顺应 世界 走向 多极化 的 趋势 ， 对于 促进 国际 社会 的 友好 合作 和 共同 发展 作出 了 积极 的 贡献</p>
<p>１９９８年，中国人民将满怀信心地开创新的业绩<br>１９９８年 ， 中国 人民 将 满怀信心 地 开创 新 的 业绩<br>１９９８年 ， 中国 人民 将 满怀信心 地 开创 新 的 业绩</p>
</blockquote>

<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>1.我们只需要修改两个地方就可以实现对bert的利用，输入格式和构造loss，其他任务也类似；</p>
<p>2.bert的中文输入格式必须是基于字的，所以在进行词性标注或者NER等任务时，也务必要以字而不是词作为基本输入单元，如果要基于词也不是不可以，那就需要自己重新预训练中文模型了；</p>
<p>想尝试的同学可以照这个思路修改下，我也会尽快开源代码和数据</p>
<p><strong>更新：</strong> 我已经上传了代码 <strong><a href="https://github.com/jiangpinglei/Bert_ChinesewordSegment" target="_blank" rel="noopener">Bert_ChinesewordSegment</a></strong></p>
<p><strong>本文系列</strong></p>
<p><a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a><br><a href="../BERT系列（二）——源码解读之模型主体">BERT系列（二）——模型主体源码解读</a><br><a href="../BERT系列（三）——源码解读之Pre-train">BERT系列（三）——源码解读之Pre-train</a><br><a href="../BERT系列（四）——源码解读之Fine-tune">BERT系列（四）——源码解读之Fine-tune</a></p>
<p><strong>Reference</strong><br>1.<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="https://jiangpinglei.github.io/BERT%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%AE%9E%E8%B7%B5-F1-97-8-%E9%99%84%E4%BB%A3%E7%A0%81/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="jiangpinglei.github.io">
  </form>
</div>


  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Jiangping Lei
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/js/jquery-3.4.1.min.js"></script>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
