<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>Tensorflow 多核GPU编程问题排查 | 西溪雷神</title>
  <meta name="author" content="Jiangping Lei">
  
  <meta name="description" content="很久没有动tensorflow了，最近实验做个分词的工具（这不是重点），以前都是在单个gpu上面运行，突然想尝试在多核GPU下跑一跑。
在网上随便找了篇帖子：https://blog.csdn.net/winycg/article/details/79759294参照着改一改，代码如下：">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Tensorflow 多核GPU编程问题排查"/>
  <meta property="og:site_name" content="西溪雷神"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="西溪雷神" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">西溪雷神</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article id="post-Tensorflow-多核GPU编程问题排查" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-11-12T10:32:38.000Z"><a href="/Tensorflow-多核GPU编程问题排查/">2018-11-12</a></time>
      
      
  
    <h1 class="p-name title" itemprop="headline name">Tensorflow 多核GPU编程问题排查</h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>很久没有动tensorflow了，最近实验做个分词的工具（这不是重点），以前都是在单个gpu上面运行，突然想尝试在多核GPU下跑一跑。</p>
<p>在网上随便找了篇帖子：<a href="https://blog.csdn.net/winycg/article/details/79759294参照着改一改，代码如下：" target="_blank" rel="noopener">https://blog.csdn.net/winycg/article/details/79759294参照着改一改，代码如下：</a></p>
<a id="more"></a>

<p>参数定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def weight_variable(shape):</span><br><span class="line">    initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">    return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">    initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">    return tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<p>bi-lstm定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def bi_lstm(X_inputs):</span><br><span class="line">    embedding &#x3D; tf.get_variable(&quot;embedding&quot;, [vocab_size, embedding_size], dtype&#x3D;tf.float32, trainable&#x3D;False)</span><br><span class="line">    # X_inputs.shape &#x3D; [batchsize, timestep_size]  -&gt;  inputs.shape &#x3D; [batchsize, timestep_size, embedding_size]</span><br><span class="line">    inputs &#x3D; tf.nn.embedding_lookup(embedding, X_inputs)</span><br><span class="line">    cell_fw &#x3D; rnn.MultiRNNCell([rnn.DropoutWrapper(cell&#x3D;tf.nn.rnn_cell.LSTMCell(hidden_size, forget_bias&#x3D;1.0, state_is_tuple&#x3D;True, name&#x3D;&#39;fw_lstm_cell&#39;), input_keep_prob&#x3D;1.0, output_keep_prob&#x3D;keep_prob) for _ in range(layer_num)], state_is_tuple&#x3D;True)</span><br><span class="line">    cell_bw &#x3D; rnn.MultiRNNCell([rnn.DropoutWrapper(cell&#x3D;tf.nn.rnn_cell.LSTMCell(hidden_size, forget_bias&#x3D;1.0, state_is_tuple&#x3D;True, name&#x3D;&#39;bw_lstm_cell&#39;), input_keep_prob&#x3D;1.0, output_keep_prob&#x3D;keep_prob) for _ in range(layer_num)], state_is_tuple&#x3D;True)</span><br><span class="line">    # **4.初始状态</span><br><span class="line">    initial_state_fw &#x3D; cell_fw.zero_state(batch_size, tf.float32)</span><br><span class="line">    initial_state_bw &#x3D; cell_fw.zero_state(batch_size, tf.float32)</span><br><span class="line">    # **5.bi-lstm 计算</span><br><span class="line">    with tf.variable_scope(&#39;bidirection_rnn&#39;):</span><br><span class="line">        # *** 下面分别计算两个网络的output 和state</span><br><span class="line">        # forward direction</span><br><span class="line">        outputs_fw &#x3D; list()</span><br><span class="line">        state_fw &#x3D; initial_state_fw</span><br><span class="line">        with tf.variable_scope(&#39;fw&#39;):</span><br><span class="line">            for timestep in range(timestep_size):</span><br><span class="line">                if timestep &gt; 0:</span><br><span class="line">                    tf.get_variable_scope().reuse_variables()</span><br><span class="line">                (output_fw, state_fw) &#x3D; cell_fw(inputs[:, timestep, :], state_fw)</span><br><span class="line">                outputs_fw.append(output_fw)</span><br><span class="line">        # backward direction</span><br><span class="line">        outputs_bw &#x3D; list()</span><br><span class="line">        state_bw &#x3D; initial_state_bw</span><br><span class="line">        with tf.variable_scope(&#39;bw&#39;):</span><br><span class="line">            inputs &#x3D; tf.reverse(inputs, [1])</span><br><span class="line">            for timestep in range(timestep_size):</span><br><span class="line">                if timestep &gt; 0:</span><br><span class="line">                    tf.get_variable_scope().reuse_variables()</span><br><span class="line">                (output_bw, state_bw) &#x3D; cell_bw(inputs[:, timestep, :], state_bw)</span><br><span class="line">                outputs_bw.append(output_bw)</span><br><span class="line">        # *** 然后把 output_bw 在 timestep 维度进行翻转</span><br><span class="line">        # 把两个oupputs 拼成 [timestep_size, batch_size, hidden_size*2]</span><br><span class="line">        output &#x3D; tf.concat([outputs_fw, outputs_bw], 2)</span><br><span class="line">        # output.shape 必须和 y_input.shape&#x3D;[batch_size,timestep_size] 对齐</span><br><span class="line">        output &#x3D; tf.transpose(output, perm&#x3D;[1, 0, 2])</span><br><span class="line">        output &#x3D; tf.reshape(output, [-1, hidden_size * 2])</span><br><span class="line">    # ***********************************************************</span><br><span class="line">    softmax_w &#x3D; weight_variable([hidden_size * 2, class_num])</span><br><span class="line">    softmax_b &#x3D; bias_variable([class_num])</span><br><span class="line">    logits &#x3D; tf.matmul(output, softmax_w) + softmax_b</span><br><span class="line">    return logits</span><br></pre></td></tr></table></figure>
<p>合并梯度：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def average_gradients(tower_grads):</span><br><span class="line">    average_grads&#x3D;[]</span><br><span class="line">    for grad_and_vars in zip(*tower_grads):</span><br><span class="line">        grads&#x3D;[]</span><br><span class="line">        for g, _ in grad_and_vars:</span><br><span class="line">            expend_g&#x3D;tf.expand_dims(g,0)</span><br><span class="line">            grads.append(expend_g)</span><br><span class="line">        grad&#x3D;tf.concat(grads,0)</span><br><span class="line">        grad&#x3D;tf.reduce_mean(grad,0)</span><br><span class="line">        v&#x3D;grad_and_vars[0][1]</span><br><span class="line">        grad_and_var&#x3D;(grad,v)</span><br><span class="line">        average_grads.append(grad_and_var)</span><br><span class="line">    return average_grads</span><br></pre></td></tr></table></figure>

<p>训练模块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def train(data_engine):</span><br><span class="line">    with tf.device(&quot;&#x2F;cpu:0&quot;):</span><br><span class="line">        tower_grads &#x3D; []</span><br><span class="line">        X_inputs &#x3D; tf.placeholder(tf.int32, [None, timestep_size], name&#x3D;&#39;X_input&#39;)</span><br><span class="line">        y_inputs &#x3D; tf.placeholder(tf.int32, [None, timestep_size], name&#x3D;&#39;y_input&#39;)</span><br><span class="line">        elr &#x3D; tf.train.exponential_decay(lr, global_step, decay_steps, decay_rate, staircase&#x3D;True, name&#x3D;None)</span><br><span class="line">        optimizer &#x3D; tf.train.AdamOptimizer(learning_rate&#x3D;elr)</span><br><span class="line">        with tf.variable_scope(tf.get_variable_scope()):</span><br><span class="line">            for i in range(gpu_nums):</span><br><span class="line">                with tf.device(&quot;&#x2F;gpu:%d&quot; % i):</span><br><span class="line">                    with tf.name_scope(&quot;tower_%d&quot; % i):</span><br><span class="line">                        _x &#x3D; X_inputs[i * batch_size:(i + 1) * batch_size]</span><br><span class="line">                        _y &#x3D; y_inputs[i * batch_size:(i + 1) * batch_size]</span><br><span class="line">                        logits &#x3D; bi_lstm(_x)</span><br><span class="line">                        loss &#x3D; tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels&#x3D;tf.reshape(_y, [-1]), logits&#x3D;logits))</span><br><span class="line">                        tf.get_variable_scope().reuse_variables()</span><br><span class="line">                        grads &#x3D; optimizer.compute_gradients(loss)</span><br><span class="line">                        tower_grads.append(grads)</span><br><span class="line">                        if i &#x3D;&#x3D; 0:</span><br><span class="line">                            logits_test &#x3D; bi_lstm(_x)</span><br><span class="line">                            test_v &#x3D; tf.cast(tf.argmax(tf.reshape(logits_test, [-1, timestep_size, class_num]), 2), tf.int32)</span><br><span class="line"></span><br><span class="line">        grads &#x3D; average_gradients(tower_grads)</span><br><span class="line">        train_op &#x3D; optimizer.apply_gradients(grads, global_step&#x3D;global_step)</span><br><span class="line">        # 梯度下降计算</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            for iteration in range(whole_epoch):</span><br><span class="line">                x1, y1 &#x3D; data_engine.train_next_batch(batch_size * gpu_nums)</span><br><span class="line">                _, t_loss &#x3D; sess.run([train_op, loss], feed_dict&#x3D;&#123;X_inputs: x1, y_inputs: y1, keep_prob: 0.5, lr: 0.01&#125;)</span><br><span class="line">                if iteration % print_step &#x3D;&#x3D; 0:</span><br><span class="line">                    print(&#39;iteration: &#39;, iteration)</span><br><span class="line">                    x2, y2 &#x3D; data_engine.validate_next_batch(batch_size)</span><br><span class="line">                    y_pre &#x3D; sess.run(test_v, feed_dict&#x3D;&#123;X_inputs: x2, y_inputs: y2, keep_prob: 1.0&#125;)</span><br><span class="line">                    print(&#39;loss: &#39;, t_loss)</span><br><span class="line">                    nozero_evaluate(y2, y_pre)</span><br></pre></td></tr></table></figure>

<p>但是在运行的时候发现梯度合并报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;run.py&quot;, line 13, in &lt;module&gt;</span><br><span class="line">    train(data_engine)</span><br><span class="line">  File &quot;&#x2F;4T&#x2F;home&#x2F;leijp&#x2F;cut2&#x2F;target&#x2F;net2.py&quot;, line 132, in train</span><br><span class="line">    grads &#x3D; average_gradients(tower_grads)</span><br><span class="line">  File &quot;&#x2F;4T&#x2F;home&#x2F;leijp&#x2F;cut2&#x2F;target&#x2F;net2.py&quot;, line 39, in average_gradients</span><br><span class="line">    expend_g&#x3D;tf.expand_dims(g,0)</span><br><span class="line">  File &quot;&#x2F;usr&#x2F;local&#x2F;anaconda3&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;tensorflow&#x2F;python&#x2F;util&#x2F;deprecation.py&quot;, line 488, in new_func</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;&#x2F;usr&#x2F;local&#x2F;anaconda3&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;tensorflow&#x2F;python&#x2F;ops&#x2F;array_ops.py&quot;, line 137, in expand_dims</span><br><span class="line">    return gen_array_ops.expand_dims(input, axis, name)</span><br><span class="line">  File &quot;&#x2F;usr&#x2F;local&#x2F;anaconda3&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;tensorflow&#x2F;python&#x2F;ops&#x2F;gen_array_ops.py&quot;, line 2088, in expand_dims</span><br><span class="line">    &quot;ExpandDims&quot;, input&#x3D;input, dim&#x3D;axis, name&#x3D;name)</span><br><span class="line">  File &quot;&#x2F;usr&#x2F;local&#x2F;anaconda3&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;tensorflow&#x2F;python&#x2F;framework&#x2F;op_def_library.py&quot;, line 528, in _apply_op_helper</span><br><span class="line">    (input_name, err))</span><br><span class="line">ValueError: Tried to convert &#39;input&#39; to a tensor and failed. Error: None values not supported.</span><br></pre></td></tr></table></figure>
<p>把梯度打印出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> (None, &lt;tf.Variable &#39;tower_0&#x2F;Variable:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32_ref&gt;),</span><br><span class="line">(None, &lt;tf.Variable &#39;tower_0&#x2F;Variable_1:0&#39; shape&#x3D;(5,) dtype&#x3D;float32_ref&gt;),</span><br><span class="line"> (None, &lt;tf.Variable &#39;tower_0&#x2F;Variable_2:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32_ref&gt;),</span><br><span class="line">  (None, &lt;tf.Variable &#39;tower_0&#x2F;Variable_3:0&#39; shape&#x3D;(5,) dtype&#x3D;float32_ref&gt;),</span><br></pre></td></tr></table></figure>
<p>发现在GPU:1计算梯度的时候，梯度竟然为None，不明所以，于是开始网上查，还好网上有一篇类似的错误：<a href="https://stackoverflow.com/questions/37593275/multi-gpu-tower-valueerror-none-values-not-supported?answertab=active#tab-top" target="_blank" rel="noopener">https://stackoverflow.com/questions/37593275/multi-gpu-tower-valueerror-none-values-not-supported?answertab=active#tab-top</a></p>
<p>下面有人回答应该是变量作用域的问题，于是我把参数定义代码改了一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def weight_variable(shape):</span><br><span class="line">    # initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">    # return tf.Variable(initial)</span><br><span class="line">    return tf.get_variable(name&#x3D;&quot;weights&quot;, shape&#x3D;shape, initializer&#x3D;tf.truncated_normal_initializer(mean&#x3D;0, stddev&#x3D;0.1))</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">    # initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">    # return tf.Variable(initial)</span><br><span class="line">    return tf.get_variable(name&#x3D;&quot;bias&quot;, shape&#x3D;shape, initializer&#x3D;tf.constant_initializer(0.1))</span><br></pre></td></tr></table></figure>
<p>再跑一次，就成功了。</p>
<p>因为之前一直是单核计算，没有涉及到多少变量重用，所以就没怎么关注作用域的问题，稀里糊涂用了这么久。后面认真的学习了下，发现一般来说，使用<code>tf.get_variable()</code>要比使用<code>tf.Variable()</code>来进行变量定义更保险，因为只要在复用代码前加一句<code>tf.get_variable_scope().reuse_variables()</code>就可以让之前定义的变量重用，这样两个GPU就能共享同一份权值。</p>
<p>后面我输出梯度的时候，变成了这样, 以”weights”为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 梯度</span><br><span class="line">GPU:0</span><br><span class="line">&lt;tf.Tensor &#39;tower_0&#x2F;gradients&#x2F;tower_0&#x2F;MatMul_grad&#x2F;tuple&#x2F;control_dependency_1:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32&gt;</span><br><span class="line">GPU:1</span><br><span class="line">&lt;tf.Tensor &#39;tower_1&#x2F;gradients&#x2F;tower_1&#x2F;MatMul_grad&#x2F;tuple&#x2F;control_dependency_1:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32&gt;</span><br><span class="line"># 权值</span><br><span class="line">GPU:0</span><br><span class="line">&lt;tf.Variable &#39;weights:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32_ref&gt;</span><br><span class="line">GPU:1</span><br><span class="line">&lt;tf.Variable &#39;weights:0&#39; shape&#x3D;(256, 5) dtype&#x3D;float32_ref&gt;</span><br></pre></td></tr></table></figure>
<p>梯度是两个不同的梯度，权值是同一份权值，这与多核GPU,数据并行的思路是一致的。</p>
<p>但为什么权值被重用了，而梯度却是各一份呢？</p>
<p>原来变量的定义是在<code>tf.variable_scope()</code>下，求解梯度过程是在<code>tf.name_scope()</code>下。</p>
<p><code>tf.variable_scope()</code>下相同的scope_name可以让变量有相同的命名，包括<code>tf.get_variable()</code>得到的变量，还有<code>tf.Variable()</code>的变量，不加<code>tf.get_variable_scope().reuse_variables()</code>的话就不能重用。</p>
<p>而<code>tf.name_scope()</code>让变量有相同的命名，只是限于<code>tf.Variable()</code>的变量，而且scope_name不同的话，定义的tf.Variable()域也会不同，所以产生的梯度自然不是同一份。</p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="https://jiangpinglei.github.io/Tensorflow-%E5%A4%9A%E6%A0%B8GPU%E7%BC%96%E7%A8%8B%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="jiangpinglei.github.io">
  </form>
</div>


  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Jiangping Lei
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/js/jquery-3.4.1.min.js"></script>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
