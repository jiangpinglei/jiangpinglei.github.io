<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>BERT系列（三）——源码解读之Pre-train | 西溪雷神</title>
  <meta name="author" content="Jiangping Lei">
  
  <meta name="description" content="pre-train是迁移学习的基础，虽然Google已经发布了各种预训练好的模型，而且因为资源消耗巨大，自己再预训练也不现实（在Google Cloud TPU v2 上训练BERT-Base要花费近500刀，耗时达到两周。在GPU上可想而知只会更贵），但是学习bert的预训练方法可以为我们弄懂整个bert的运行流程提供莫大的帮助。预训练涉及到的模块有点多，所以这也将会是一篇长文，在能简略的地方我尽量简略，还是那句话，我的文章只能是起到一个导读的作用，如果想摸清里面的各种细节还是要自己把源码过一遍的。
pre-train涉及到的模块分为以下三个，我将为大家一一介绍：

tokenization.py

create_pretraining_data.py

run_pretraining.py


其中tokenization是对原始句子内容的解析，分为BasicTokenizer和WordpieceTokenizer两个，不只是在预训练中，在fine-tune和推断过程同样要用到它；create_pretraining_data顾名思义就是将原始语料转换成适合模型预训练的输入数据；run_pretraining就是预训练的执行代码了。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="BERT系列（三）——源码解读之Pre-train"/>
  <meta property="og:site_name" content="西溪雷神"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="西溪雷神" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">西溪雷神</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article id="post-BERT系列（三）——源码解读之Pre-train" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-12-25T09:58:24.000Z"><a href="/BERT系列（三）——源码解读之Pre-train/">2018-12-25</a></time>
      
      
  
    <h1 class="p-name title" itemprop="headline name">BERT系列（三）——源码解读之Pre-train</h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>pre-train是迁移学习的基础，虽然Google已经发布了各种预训练好的模型，而且因为资源消耗巨大，自己再预训练也不现实（在Google Cloud TPU v2 上训练BERT-Base要花费近500刀，耗时达到两周。在GPU上可想而知只会更贵），但是学习bert的预训练方法可以为我们弄懂整个bert的运行流程提供莫大的帮助。预训练涉及到的模块有点多，所以这也将会是一篇长文，在能简略的地方我尽量简略，还是那句话，我的文章只能是起到一个导读的作用，如果想摸清里面的各种细节还是要自己把源码过一遍的。</p>
<p>pre-train涉及到的模块分为以下三个，我将为大家一一介绍：</p>
<ol>
<li><p><a href="https://github.com/google-research/bert/blob/master/tokenization.py" target="_blank" rel="noopener" title="tokenization.py">tokenization.py</a></p>
</li>
<li><p><a href="https://github.com/google-research/bert/blob/master/create_pretraining_data.py" target="_blank" rel="noopener" title="create_pretraining_data.py">create_pretraining_data.py</a></p>
</li>
<li><p><a href="https://github.com/google-research/bert/blob/master/run_pretraining.py" target="_blank" rel="noopener" title="run_pretraining.py">run_pretraining.py</a></p>
</li>
</ol>
<p>其中tokenization是对原始句子内容的解析，分为BasicTokenizer和WordpieceTokenizer两个，不只是在预训练中，在fine-tune和推断过程同样要用到它；create_pretraining_data顾名思义就是将原始语料转换成适合模型预训练的输入数据；run_pretraining就是预训练的执行代码了。</p>
<a id="more"></a>

<h1 id="一、tokenization-py"><a href="#一、tokenization-py" class="headerlink" title="一、tokenization.py"></a>一、tokenization.py</h1><h2 id="1、BasicTokenizer"><a href="#1、BasicTokenizer" class="headerlink" title="1、BasicTokenizer"></a>1、BasicTokenizer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">class BasicTokenizer(object):</span><br><span class="line">  &quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self, do_lower_case&#x3D;True):</span><br><span class="line">    self.do_lower_case &#x3D; do_lower_case</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    &quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;</span><br><span class="line">    text &#x3D; convert_to_unicode(text)</span><br><span class="line">    text &#x3D; self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    text &#x3D; self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens &#x3D; whitespace_tokenize(text)</span><br><span class="line">    split_tokens &#x3D; []</span><br><span class="line">    for token in orig_tokens:</span><br><span class="line">      if self.do_lower_case:</span><br><span class="line">        token &#x3D; token.lower()</span><br><span class="line">        token &#x3D; self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens &#x3D; whitespace_tokenize(&quot; &quot;.join(split_tokens))</span><br><span class="line">    return output_tokens</span><br><span class="line"></span><br><span class="line">  def _run_strip_accents(self, text):</span><br><span class="line">    &quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span><br><span class="line">    text &#x3D; unicodedata.normalize(&quot;NFD&quot;, text)</span><br><span class="line">    output &#x3D; []</span><br><span class="line">    for char in text:</span><br><span class="line">      cat &#x3D; unicodedata.category(char)</span><br><span class="line">      if cat &#x3D;&#x3D; &quot;Mn&quot;:</span><br><span class="line">        continue</span><br><span class="line">      output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _run_split_on_punc(self, text):</span><br><span class="line">    &quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span><br><span class="line">    chars &#x3D; list(text)</span><br><span class="line">    i &#x3D; 0</span><br><span class="line">    start_new_word &#x3D; True</span><br><span class="line">    output &#x3D; []</span><br><span class="line">    while i &lt; len(chars):</span><br><span class="line">      char &#x3D; chars[i]</span><br><span class="line">      if _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word &#x3D; True</span><br><span class="line">      else:</span><br><span class="line">        if start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word &#x3D; False</span><br><span class="line">        output[-1].append(char)</span><br><span class="line">      i +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    return [&quot;&quot;.join(x) for x in output]</span><br><span class="line"></span><br><span class="line">  def _tokenize_chinese_chars(self, text):</span><br><span class="line">    &quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span><br><span class="line">    output &#x3D; []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp &#x3D; ord(char)</span><br><span class="line">      if self._is_chinese_char(cp):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _is_chinese_char(self, cp):</span><br><span class="line">    &quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span><br><span class="line">    if ((cp &gt;&#x3D; 0x4E00 and cp &lt;&#x3D; 0x9FFF) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x3400 and cp &lt;&#x3D; 0x4DBF) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x20000 and cp &lt;&#x3D; 0x2A6DF) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x2A700 and cp &lt;&#x3D; 0x2B73F) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x2B740 and cp &lt;&#x3D; 0x2B81F) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x2B820 and cp &lt;&#x3D; 0x2CEAF) or</span><br><span class="line">        (cp &gt;&#x3D; 0xF900 and cp &lt;&#x3D; 0xFAFF) or  #</span><br><span class="line">        (cp &gt;&#x3D; 0x2F800 and cp &lt;&#x3D; 0x2FA1F)):  #</span><br><span class="line">      return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">  def _clean_text(self, text):</span><br><span class="line">    &quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span><br><span class="line">    output &#x3D; []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp &#x3D; ord(char)</span><br><span class="line">      if cp &#x3D;&#x3D; 0 or cp &#x3D;&#x3D; 0xfffd or _is_control(char):</span><br><span class="line">        continue</span><br><span class="line">      if _is_whitespace(char):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br></pre></td></tr></table></figure>
<p>BasicTokenizer的主要是进行unicode转换、标点符号分割、小写转换、中文字符分割、去除重音符号等操作，最后返回的是关于词的数组（中文是字的数组）</p>
<h2 id="2、WordpieceTokenizer"><a href="#2、WordpieceTokenizer" class="headerlink" title="2、WordpieceTokenizer"></a>2、WordpieceTokenizer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">class WordpieceTokenizer(object):</span><br><span class="line">  &quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self, vocab, unk_token&#x3D;&quot;[UNK]&quot;, max_input_chars_per_word&#x3D;200):</span><br><span class="line">    self.vocab &#x3D; vocab</span><br><span class="line">    self.unk_token &#x3D; unk_token</span><br><span class="line">    self.max_input_chars_per_word &#x3D; max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    text &#x3D; convert_to_unicode(text)</span><br><span class="line">    output_tokens &#x3D; []</span><br><span class="line">    for token in whitespace_tokenize(text):</span><br><span class="line">      chars &#x3D; list(token)</span><br><span class="line">      if len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        continue</span><br><span class="line">      is_bad &#x3D; False</span><br><span class="line">      start &#x3D; 0</span><br><span class="line">      sub_tokens &#x3D; []</span><br><span class="line">      while start &lt; len(chars):</span><br><span class="line">        end &#x3D; len(chars)</span><br><span class="line">        cur_substr &#x3D; None</span><br><span class="line">        while start &lt; end:</span><br><span class="line">          substr &#x3D; &quot;&quot;.join(chars[start:end])</span><br><span class="line">          if start &gt; 0:</span><br><span class="line">            substr &#x3D; &quot;##&quot; + substr</span><br><span class="line">          if substr in self.vocab:</span><br><span class="line">            cur_substr &#x3D; substr</span><br><span class="line">            break</span><br><span class="line">          end -&#x3D; 1</span><br><span class="line">        if cur_substr is None:</span><br><span class="line">          is_bad &#x3D; True</span><br><span class="line">          break</span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start &#x3D; end</span><br><span class="line"></span><br><span class="line">      if is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      else:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    return output_tokens</span><br></pre></td></tr></table></figure>
<p>WordpieceTokenizer的目的是将合成词分解成类似词根一样的词片。例如将”unwanted”分解成[“un”, “##want”, “##ed”]这么做的目的是防止因为词的过于生僻没有被收录进词典最后只能以[UNK]代替的局面，因为英语当中这样的合成词非常多，词典不可能全部收录。</p>
<h2 id="3、FullTokenizer"><a href="#3、FullTokenizer" class="headerlink" title="3、FullTokenizer"></a>3、FullTokenizer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class FullTokenizer(object):</span><br><span class="line">  &quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self, vocab_file, do_lower_case&#x3D;True):</span><br><span class="line">    self.vocab &#x3D; load_vocab(vocab_file)</span><br><span class="line">    self.inv_vocab &#x3D; &#123;v: k for k, v in self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer &#x3D; BasicTokenizer(do_lower_case&#x3D;do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer &#x3D; WordpieceTokenizer(vocab&#x3D;self.vocab)</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    split_tokens &#x3D; []</span><br><span class="line">    for token in self.basic_tokenizer.tokenize(text):</span><br><span class="line">      for sub_token in self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    return split_tokens</span><br><span class="line"></span><br><span class="line">  def convert_tokens_to_ids(self, tokens):</span><br><span class="line">    return convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  def convert_ids_to_tokens(self, ids):</span><br><span class="line">    return convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>

<p>FullTokenizer的作用就很显而易见了，对一个文本段进行以上两种解析，最后返回词（字）的数组，同时还提供token到id的索引以及id到token的索引。这里的token可以理解为文本段处理过后的最小单元。</p>
<h1 id="二、create-pretraining-data-py"><a href="#二、create-pretraining-data-py" class="headerlink" title="二、create_pretraining_data.py"></a>二、create_pretraining_data.py</h1><h2 id="1、配置"><a href="#1、配置" class="headerlink" title="1、配置"></a>1、配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">flags.DEFINE_string(&quot;input_file&quot;, None,</span><br><span class="line">                    &quot;Input raw text file (or comma-separated list of files).&quot;)</span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    &quot;output_file&quot;, None,</span><br><span class="line">    &quot;Output TF example file (or comma-separated list of files).&quot;)</span><br><span class="line">flags.DEFINE_string(&quot;vocab_file&quot;, None,</span><br><span class="line">                    &quot;The vocabulary file that the BERT model was trained on.&quot;)</span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    &quot;do_lower_case&quot;, True,</span><br><span class="line">    &quot;Whether to lower case the input text. Should be True for uncased &quot;</span><br><span class="line">    &quot;models and False for cased models.&quot;)</span><br><span class="line">flags.DEFINE_integer(&quot;max_seq_length&quot;, 128, &quot;Maximum sequence length.&quot;)</span><br><span class="line">flags.DEFINE_integer(&quot;max_predictions_per_seq&quot;, 20,</span><br><span class="line">                     &quot;Maximum number of masked LM predictions per sequence.&quot;)</span><br><span class="line">flags.DEFINE_integer(&quot;random_seed&quot;, 12345, &quot;Random seed for data generation.&quot;)</span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    &quot;dupe_factor&quot;, 10,</span><br><span class="line">    &quot;Number of times to duplicate the input data (with different masks).&quot;)</span><br><span class="line">flags.DEFINE_float(&quot;masked_lm_prob&quot;, 0.15, &quot;Masked LM probability.&quot;)</span><br><span class="line">flags.DEFINE_float(</span><br><span class="line">    &quot;short_seq_prob&quot;, 0.1,</span><br><span class="line">    &quot;Probability of creating sequences which are shorter than the &quot;</span><br><span class="line">    &quot;maximum length.&quot;)</span><br></pre></td></tr></table></figure>
<p>配置input_file、output_file分别代表输入的源语料文件和处理过的预料文件地址；</p>
<p>do_lower_case：是否全部转为小写字母，是否转换成小写字母的意义在<a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a>里面已经说过了。</p>
<p>dupe_factor：默认重复10次，目的是可以生成不同情况的masks；</p>
<p>short_seq_prob：构造长度小于指定”max_seq_length”的样本比例。因为在fine-tune过程里面输入的target_seq_length是可变的（小于等于max_seq_length），那么为了防止过拟合也需要在pre-train的过程当中构造一些短的样本。</p>
<h1 id="2、main入口"><a href="#2、main入口" class="headerlink" title="2、main入口"></a>2、main入口</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def main(_):</span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  tokenizer &#x3D; tokenization.FullTokenizer(</span><br><span class="line">      vocab_file&#x3D;FLAGS.vocab_file, do_lower_case&#x3D;FLAGS.do_lower_case)</span><br><span class="line"></span><br><span class="line">  input_files &#x3D; []</span><br><span class="line">  for input_pattern in FLAGS.input_file.split(&quot;,&quot;):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(&quot;*** Reading from input files ***&quot;)</span><br><span class="line">  for input_file in input_files:</span><br><span class="line">    tf.logging.info(&quot;  %s&quot;, input_file)</span><br><span class="line"></span><br><span class="line">  rng &#x3D; random.Random(FLAGS.random_seed)</span><br><span class="line">  instances &#x3D; create_training_instances(</span><br><span class="line">      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,</span><br><span class="line">      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,</span><br><span class="line">      rng)</span><br><span class="line"></span><br><span class="line">  output_files &#x3D; FLAGS.output_file.split(&quot;,&quot;)</span><br><span class="line">  tf.logging.info(&quot;*** Writing to output files ***&quot;)</span><br><span class="line">  for output_file in output_files:</span><br><span class="line">    tf.logging.info(&quot;  %s&quot;, output_file)</span><br><span class="line"></span><br><span class="line">  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,</span><br><span class="line">                                  FLAGS.max_predictions_per_seq, output_files)</span><br></pre></td></tr></table></figure>
<p>从入口开始看，步骤很简单：1）构造tokenizer ；2）构造instances ；3）保存instances </p>
<h2 id="3、构造instances"><a href="#3、构造instances" class="headerlink" title="3、构造instances"></a>3、构造instances</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def create_training_instances(input_files, tokenizer, max_seq_length,</span><br><span class="line">                              dupe_factor, short_seq_prob, masked_lm_prob,</span><br><span class="line">                              max_predictions_per_seq, rng):</span><br><span class="line">  &quot;&quot;&quot;Create &#96;TrainingInstance&#96;s from raw text.&quot;&quot;&quot;</span><br><span class="line">  all_documents &#x3D; [[]]</span><br><span class="line">  for input_file in input_files:</span><br><span class="line">    with tf.gfile.GFile(input_file, &quot;r&quot;) as reader:</span><br><span class="line">      while True:</span><br><span class="line">        line &#x3D; tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">        if not line:</span><br><span class="line">          break</span><br><span class="line">        line &#x3D; line.strip()</span><br><span class="line"></span><br><span class="line">        # Empty lines are used as document delimiters</span><br><span class="line">        if not line:</span><br><span class="line">          all_documents.append([])</span><br><span class="line">        tokens &#x3D; tokenizer.tokenize(line)</span><br><span class="line">        if tokens:</span><br><span class="line">          all_documents[-1].append(tokens)</span><br><span class="line"></span><br><span class="line">  # Remove empty documents</span><br><span class="line">  all_documents &#x3D; [x for x in all_documents if x]</span><br><span class="line">  rng.shuffle(all_documents)</span><br><span class="line"></span><br><span class="line">  vocab_words &#x3D; list(tokenizer.vocab.keys())</span><br><span class="line">  instances &#x3D; []</span><br><span class="line">  for _ in range(dupe_factor):</span><br><span class="line">    for document_index in range(len(all_documents)):</span><br><span class="line">      instances.extend(</span><br><span class="line">          create_instances_from_document(</span><br><span class="line">              all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))</span><br><span class="line"></span><br><span class="line">  rng.shuffle(instances)</span><br><span class="line">  return instances</span><br></pre></td></tr></table></figure>
<p>这一步是阅读数据，数据的输入文本可以是一个文件也可以是用逗号分割的若干文件；<br>文件里用换行来表示句子的边界，即一句一行，同理段落之间用空一行来表示段落的边界，一个段落表示成一个document；具体的构造方法在create_instances_from_document函数里面。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">def create_instances_from_document(</span><br><span class="line">    all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">  &quot;&quot;&quot;Creates &#96;TrainingInstance&#96;s for a single document.&quot;&quot;&quot;</span><br><span class="line">  document &#x3D; all_documents[document_index]</span><br><span class="line"></span><br><span class="line">  # Account for [CLS], [SEP], [SEP]</span><br><span class="line">  max_num_tokens &#x3D; max_seq_length - 3</span><br><span class="line">  target_seq_length &#x3D; max_num_tokens</span><br><span class="line">  if rng.random() &lt; short_seq_prob:</span><br><span class="line">    target_seq_length &#x3D; rng.randint(2, max_num_tokens)</span><br><span class="line"></span><br><span class="line">  instances &#x3D; []</span><br><span class="line">  current_chunk &#x3D; []</span><br><span class="line">  current_length &#x3D; 0</span><br><span class="line">  i &#x3D; 0</span><br><span class="line">  while i &lt; len(document):</span><br><span class="line">    segment &#x3D; document[i]</span><br><span class="line">    current_chunk.append(segment)</span><br><span class="line">    current_length +&#x3D; len(segment)</span><br><span class="line">    if i &#x3D;&#x3D; len(document) - 1 or current_length &gt;&#x3D; target_seq_length:</span><br><span class="line">      if current_chunk:</span><br><span class="line">        # &#96;a_end&#96; is how many segments from &#96;current_chunk&#96; go into the &#96;A&#96;</span><br><span class="line">        # (first) sentence.</span><br><span class="line">        a_end &#x3D; 1</span><br><span class="line">        if len(current_chunk) &gt;&#x3D; 2:</span><br><span class="line">          a_end &#x3D; rng.randint(1, len(current_chunk) - 1)</span><br><span class="line"></span><br><span class="line">        tokens_a &#x3D; []</span><br><span class="line">        for j in range(a_end):</span><br><span class="line">          tokens_a.extend(current_chunk[j])</span><br><span class="line"></span><br><span class="line">        tokens_b &#x3D; []</span><br><span class="line">        # Random next</span><br><span class="line">        is_random_next &#x3D; False</span><br><span class="line">        if len(current_chunk) &#x3D;&#x3D; 1 or rng.random() &lt; 0.5:</span><br><span class="line">          is_random_next &#x3D; True</span><br><span class="line">          target_b_length &#x3D; target_seq_length - len(tokens_a)</span><br><span class="line"></span><br><span class="line">          for _ in range(10):</span><br><span class="line">            random_document_index &#x3D; rng.randint(0, len(all_documents) - 1)</span><br><span class="line">            if random_document_index !&#x3D; document_index:</span><br><span class="line">              break</span><br><span class="line"></span><br><span class="line">          random_document &#x3D; all_documents[random_document_index]</span><br><span class="line">          random_start &#x3D; rng.randint(0, len(random_document) - 1)</span><br><span class="line">          for j in range(random_start, len(random_document)):</span><br><span class="line">            tokens_b.extend(random_document[j])</span><br><span class="line">            if len(tokens_b) &gt;&#x3D; target_b_length:</span><br><span class="line">              break</span><br><span class="line">  </span><br><span class="line">          num_unused_segments &#x3D; len(current_chunk) - a_end</span><br><span class="line">          i -&#x3D; num_unused_segments</span><br><span class="line">        # Actual next</span><br><span class="line">        else:</span><br><span class="line">          is_random_next &#x3D; False</span><br><span class="line">          for j in range(a_end, len(current_chunk)):</span><br><span class="line">            tokens_b.extend(current_chunk[j])</span><br><span class="line">        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line"></span><br><span class="line">        assert len(tokens_a) &gt;&#x3D; 1</span><br><span class="line">        assert len(tokens_b) &gt;&#x3D; 1</span><br><span class="line"></span><br><span class="line">        tokens &#x3D; []</span><br><span class="line">        segment_ids &#x3D; []</span><br><span class="line">        tokens.append(&quot;[CLS]&quot;)</span><br><span class="line">        segment_ids.append(0)</span><br><span class="line">        for token in tokens_a:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(0)</span><br><span class="line"></span><br><span class="line">        tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">        segment_ids.append(0)</span><br><span class="line"></span><br><span class="line">        for token in tokens_b:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(1)</span><br><span class="line">        tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">        segment_ids.append(1)</span><br><span class="line"></span><br><span class="line">        (tokens, masked_lm_positions,</span><br><span class="line">         masked_lm_labels) &#x3D; create_masked_lm_predictions(</span><br><span class="line">             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br><span class="line">        instance &#x3D; TrainingInstance(</span><br><span class="line">            tokens&#x3D;tokens,</span><br><span class="line">            segment_ids&#x3D;segment_ids,</span><br><span class="line">            is_random_next&#x3D;is_random_next,</span><br><span class="line">            masked_lm_positions&#x3D;masked_lm_positions,</span><br><span class="line">            masked_lm_labels&#x3D;masked_lm_labels)</span><br><span class="line">        instances.append(instance)</span><br><span class="line">      current_chunk &#x3D; []</span><br><span class="line">      current_length &#x3D; 0</span><br><span class="line">    i +&#x3D; 1</span><br><span class="line">  return instances</span><br></pre></td></tr></table></figure>
<p>这一段算是整个模块的核心了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">instance &#x3D; TrainingInstance(</span><br><span class="line">            tokens&#x3D;tokens,</span><br><span class="line">            segment_ids&#x3D;segment_ids,</span><br><span class="line">            is_random_next&#x3D;is_random_next,</span><br><span class="line">            masked_lm_positions&#x3D;masked_lm_positions,</span><br><span class="line">            masked_lm_labels&#x3D;masked_lm_labels)</span><br></pre></td></tr></table></figure>
<p>1）一个instance 包含一个tokens，实际上就是输入的词序列；该序列表现形式为：</p>
<p><strong>[CLS] A [SEP] B [SEP]</strong></p>
<p><strong>A=[token_0, token_1, …,token_i]</strong><br><strong>B=[token_i+1, token_i+2, …,token_n-1]</strong></p>
<p>其中:<br>2&lt;= n &lt; max_seq_length - 3 (in short_seq_prob)<br>n=max_seq_length - 3 (in 1-short_seq_prob)</p>
<p>token 最后表现形式如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/14927967-142fad692781a79e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="tokens示意图"></p>
<p>segment_ids 指的形式为[0,0,0…1,1,111] 0的个数为i+1个，1的个数为max_seq_length - (i+1)<br>对应到模型输入就是token_type</p>
<p>is_random_next：其实就是上图的Label，0.5的概率为True（和当只有一个segment的时候），如果为True则B和A不属于同一document。剩下的情况为False，则B为A同一document的后续句子。</p>
<p>masked_lm_positions：序列里被[MASK]的位置；</p>
<p>masked_lm_labels：序列里被[MASK]的token</p>
<p>2）在create_masked_lm_predictions函数里，一个序列在指定MASK数量之后，有80%被真正MASK，10%还是保留原来token，10%被随机替换成其他token。</p>
<h2 id="4、保存instance"><a href="#4、保存instance" class="headerlink" title="4、保存instance"></a>4、保存instance</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">def write_instance_to_example_files(instances, tokenizer, max_seq_length,</span><br><span class="line">                                    max_predictions_per_seq, output_files):</span><br><span class="line">  &quot;&quot;&quot;Create TF example files from &#96;TrainingInstance&#96;s.&quot;&quot;&quot;</span><br><span class="line">  writers &#x3D; []</span><br><span class="line">  for output_file in output_files:</span><br><span class="line">    writers.append(tf.python_io.TFRecordWriter(output_file))</span><br><span class="line"></span><br><span class="line">  writer_index &#x3D; 0</span><br><span class="line"></span><br><span class="line">  total_written &#x3D; 0</span><br><span class="line">  for (inst_index, instance) in enumerate(instances):</span><br><span class="line">    input_ids &#x3D; tokenizer.convert_tokens_to_ids(instance.tokens)</span><br><span class="line">    input_mask &#x3D; [1] * len(input_ids)</span><br><span class="line">    segment_ids &#x3D; list(instance.segment_ids)</span><br><span class="line">    assert len(input_ids) &lt;&#x3D; max_seq_length</span><br><span class="line"></span><br><span class="line">    while len(input_ids) &lt; max_seq_length:</span><br><span class="line">      input_ids.append(0)</span><br><span class="line">      input_mask.append(0)</span><br><span class="line">      segment_ids.append(0)</span><br><span class="line"></span><br><span class="line">    assert len(input_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    assert len(input_mask) &#x3D;&#x3D; max_seq_length</span><br><span class="line">    assert len(segment_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line"></span><br><span class="line">    masked_lm_positions &#x3D; list(instance.masked_lm_positions)</span><br><span class="line">    masked_lm_ids &#x3D; tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)</span><br><span class="line">    masked_lm_weights &#x3D; [1.0] * len(masked_lm_ids)</span><br><span class="line"></span><br><span class="line">    while len(masked_lm_positions) &lt; max_predictions_per_seq:</span><br><span class="line">      masked_lm_positions.append(0)</span><br><span class="line">      masked_lm_ids.append(0)</span><br><span class="line">      masked_lm_weights.append(0.0)</span><br><span class="line"></span><br><span class="line">    next_sentence_label &#x3D; 1 if instance.is_random_next else 0</span><br><span class="line"></span><br><span class="line">    features &#x3D; collections.OrderedDict()</span><br><span class="line">    features[&quot;input_ids&quot;] &#x3D; create_int_feature(input_ids)</span><br><span class="line">    features[&quot;input_mask&quot;] &#x3D; create_int_feature(input_mask)</span><br><span class="line">    features[&quot;segment_ids&quot;] &#x3D; create_int_feature(segment_ids)</span><br><span class="line">    features[&quot;masked_lm_positions&quot;] &#x3D; create_int_feature(masked_lm_positions)</span><br><span class="line">    features[&quot;masked_lm_ids&quot;] &#x3D; create_int_feature(masked_lm_ids)</span><br><span class="line">    features[&quot;masked_lm_weights&quot;] &#x3D; create_float_feature(masked_lm_weights)</span><br><span class="line">    features[&quot;next_sentence_labels&quot;] &#x3D; create_int_feature([next_sentence_label])</span><br><span class="line"></span><br><span class="line">    tf_example &#x3D; tf.train.Example(features&#x3D;tf.train.Features(feature&#x3D;features))</span><br><span class="line"></span><br><span class="line">    writers[writer_index].write(tf_example.SerializeToString())</span><br><span class="line">    writer_index &#x3D; (writer_index + 1) % len(writers)</span><br><span class="line"></span><br><span class="line">    total_written +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    if inst_index &lt; 20:</span><br><span class="line">      tf.logging.info(&quot;*** Example ***&quot;)</span><br><span class="line">      tf.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join(</span><br><span class="line">          [tokenization.printable_text(x) for x in instance.tokens]))</span><br><span class="line"></span><br><span class="line">      for feature_name in features.keys():</span><br><span class="line">        feature &#x3D; features[feature_name]</span><br><span class="line">        values &#x3D; []</span><br><span class="line">        if feature.int64_list.value:</span><br><span class="line">          values &#x3D; feature.int64_list.value</span><br><span class="line">        elif feature.float_list.value:</span><br><span class="line">          values &#x3D; feature.float_list.value</span><br><span class="line">        tf.logging.info(</span><br><span class="line">            &quot;%s: %s&quot; % (feature_name, &quot; &quot;.join([str(x) for x in values])))</span><br><span class="line"></span><br><span class="line">  for writer in writers:</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">  tf.logging.info(&quot;Wrote %d total instances&quot;, total_written)</span><br></pre></td></tr></table></figure>
<p>instance保存没什么好说的，只有两点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while len(input_ids) &lt; max_seq_length:</span><br><span class="line">      input_ids.append(0)</span><br><span class="line">      input_mask.append(0)</span><br><span class="line">      segment_ids.append(0)</span><br></pre></td></tr></table></figure>
<p>1）之前不是有short_seq_prob的概率导致样本的长度小于max_predictions_per_seq吗，这里把这些样本补齐，padding为0，同样的还有input_mask和segment_ids；<br>2）把instance的is_random_next转化成变量next_sentence_label保存。</p>
<p>为了验证这个数据模块对中文输入输出的支持，我做了个测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 create_pretraining_data.py   --input_file&#x3D;&#x2F;tmp&#x2F;zh_test.txt   --output_file&#x3D;&#x2F;tmp&#x2F;output.txt   --vocab_file&#x3D;$BERT_ZH_DIR&#x2F;vocab.txt</span><br></pre></td></tr></table></figure>
<p>zh_test.txt是我脸滚键盘随意输入的一些汉字，共有两段，每段两句话：</p>
<blockquote><p>酒店附近开房的艰苦的飞机飞抵发窘惹风波，觉得覅奇偶均衡能否v不。<br>极度疯狂减肥的人能否打开v高科技就而后就覅哦冏结构i恶如桂萼黑人牙膏覅u我也【发票未开u俄日附件二我就佛i额外阶级感v，我为何军方的我i和服i好热哦iu均为辐9为u</p>
<p>ui和覅文化覅哦佛为进度覅u蛊蛾i巨乳古人规格i兼顾如果我是破看到v个ui就火热i今年的付款了几个vi哦素问。就觉发给金佛i为借口破碎的梦<br>i觉得覅u而非各位i风格较为哦个粉色哦i多发几个v二哥i文件哦i怪兽决斗盘可加热管覅u个人文集狗哥</p>
</blockquote>


<p>vocab.txt是下载的bert中文预训练模型里的词典</p>
<p>最后的部分输出如下所示：</p>
<blockquote><p>INFO:tensorflow:<strong>* Example *</strong><br>INFO:tensorflow:tokens: [CLS] i 觉 得 [UNK] u [MASK] 非 [MASK] 位 i 风 格 较 ##by 哦 个 驅 色 哦 i 多 发 [MASK] 个 v 二 哥 i 文 件 哦 i 怪 [MASK] 决 斗 盘 可 加 热 管 [MASK] u [MASK] [MASK] 文 集 狗 哥 [SEP] [MASK] [UNK] 奇 偶 均 衡 能 否 v 不 。 极 [MASK] 疯 狂 减 肥 的 人 能 否 打 开 v 高 科 技 就 而 [MASK] 就 [UNK] 哦 冏 结 构 i 恶 如 桂 萼 黑 人 牙 膏 [UNK] u 我 也 【 发 票 未 开 [MASK] 俄 日 [MASK] 件 二 我 就 佛 i 额 [MASK] 阶 [MASK] 感 v [MASK] 我 为 [MASK] 军 方 [SEP]<br>INFO:tensorflow:input_ids: 101 151 6230 2533 100 163 103 7478 103 855 151 7599 3419 6772 8684 1521 702 7705 5682 1521 151 1914 1355 103 702 164 753 1520 151 3152 816 1521 151 2597 103 1104 3159 4669 1377 1217 4178 5052 103 163 103 103 3152 7415 4318 1520 102 103 100 1936 981 1772 6130 5543 1415 164 679 511 3353 103 4556 4312 1121 5503 4638 782 5543 1415 2802 2458 164 7770 4906 2825 2218 5445 103 2218 100 1521 1087 5310 3354 151 2626 1963 3424 5861 7946 782 4280 5601 100 163 2769 738 523 1355 4873 3313 2458 103 915 3189 103 816 753 2769 2218 867 151 7583 103 7348 103 2697 164 103 2769 711 103 1092 3175 102<br>INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1<br>INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1<br>INFO:tensorflow:masked_lm_positions: 6 8 14 17 23 34 42 44 45 46 51 63 80 105 108 116 118 121 124 0<br>INFO:tensorflow:masked_lm_ids: 5445 1392 711 5106 1126 1077 100 702 782 3152 2533 2428 1400 163 7353 1912 5277 8024 862 0<br>INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0<br>INFO:tensorflow:next_sentence_labels: 1</p>
</blockquote>

<p>可以看到token序列里的中文确实是以字的形式出现的</p>
<h1 id="三、run-pretraining-py"><a href="#三、run-pretraining-py" class="headerlink" title="三、run_pretraining.py"></a>三、run_pretraining.py</h1><p>终于到预训练的执行模块了，里面大部分都是tensorflow训练的常规代码，感觉没什么好分析的。</p>
<p>看过前面的内容和我前两章内容的朋友我想已经初步知道预训练的整个逻辑了,这里作一个简单的介绍：</p>
<h2 id="1、X和Y的确定"><a href="#1、X和Y的确定" class="headerlink" title="1、X和Y的确定"></a>1、X和Y的确定</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input_ids &#x3D; features[&quot;input_ids&quot;]</span><br><span class="line">input_mask &#x3D; features[&quot;input_mask&quot;]</span><br><span class="line">segment_ids &#x3D; features[&quot;segment_ids&quot;]</span><br><span class="line">masked_lm_positions &#x3D; features[&quot;masked_lm_positions&quot;]</span><br><span class="line">masked_lm_ids &#x3D; features[&quot;masked_lm_ids&quot;]</span><br><span class="line">masked_lm_weights &#x3D; features[&quot;masked_lm_weights&quot;]</span><br><span class="line">next_sentence_labels &#x3D; features[&quot;next_sentence_labels&quot;]</span><br><span class="line">model &#x3D; modeling.BertModel(</span><br><span class="line">    config&#x3D;bert_config,</span><br><span class="line">    is_training&#x3D;is_training,</span><br><span class="line">    input_ids&#x3D;input_ids,</span><br><span class="line">    input_mask&#x3D;input_mask,</span><br><span class="line">    token_type_ids&#x3D;segment_ids,</span><br><span class="line">    use_one_hot_embeddings&#x3D;use_one_hot_embeddings)</span><br></pre></td></tr></table></figure>
<p>其中input_ids、input_mask 、segment_ids 作为X，剩下的masked_lm_positions、masked_lm_ids 、masked_lm_weights 、next_sentence_labels 共同作为Y</p>
<h2 id="2、-loss"><a href="#2、-loss" class="headerlink" title="2、 loss"></a>2、 loss</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(masked_lm_loss,</span><br><span class="line"> masked_lm_example_loss, masked_lm_log_probs) &#x3D; get_masked_lm_output(</span><br><span class="line">     bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">     masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">(next_sentence_loss, next_sentence_example_loss,</span><br><span class="line"> next_sentence_log_probs) &#x3D; get_next_sentence_output(</span><br><span class="line">     bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">total_loss &#x3D; masked_lm_loss + next_sentence_loss</span><br></pre></td></tr></table></figure>

<p>可以看到loss 分别由masked_lm_loss和next_sentence_loss组成，masked_lm_loss针对的是语言模型对MASK起来的标签的预测，即上下文语境预测当前词；而next_sentence_loss是对于句子关系的预测。前者在迁移学习中可以用于标注类任务（分词、NER等），后者可以用于句子关系任务（QA、自然语言推理等）。</p>
<p>需要多说一句的是，masked_lm_loss，用到了模型的sequence_output和embedding_table，这是因为对多个MASK的标签进行预测是一个标注问题，所以需要获取最后一层的整个sequence，而embedding_table用来反embedding，这样就映射到token的学习了。而next_sentence_loss用到的是pooled_output，对应的是第一个token [CLS]，它一般用于分类任务的学习。</p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>本文介绍了以下几个内容：</p>
<p>1、tokenization模块：我把它叫做对原始文本段的解析，只有解析过后才能标准化输入；</p>
<p>2、create_pretraining_data模块：对原始数据进行转换，原始数据本是无标签的数据，通过句子的拼接可以产生句子关系的标签，通过MASK可以产生标注的标签，其本质是语言模型的应用；</p>
<p>3、run_pretraining模块：在执行预训练的时候针对以上两种标签分别利用bert模型的不同输出部件，计算loss，然后进行梯度下降优化。</p>
<p><strong>本文系列</strong><br><a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a><br><a href="../BERT系列（二）——源码解读之模型主体">BERT系列（二）——模型主体源码解读</a><br><a href="../BERT系列（四）——源码解读之Fine-tune">BERT系列（四）——源码解读之Fine-tune</a><br><a href="../BERT系列（五）——中文分词实践-F1-97-8-附代码">BERT系列（五）——中文分词实践 F1 97.8%(附代码)</a></p>
<p><strong>Reference</strong><br>1.<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a><br>2.<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding</a></p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="https://jiangpinglei.github.io/BERT%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BPre-train/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="jiangpinglei.github.io">
  </form>
</div>


  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Jiangping Lei
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/js/jquery-3.4.1.min.js"></script>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
