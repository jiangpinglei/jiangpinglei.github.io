<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>BERT系列（二）——源码解读之模型主体 | 西溪雷神</title>
  <meta name="author" content="Jiangping Lei">
  
  <meta name="description" content="本篇文章主要是解读模型主体代码modeling.py。在阅读这篇文章之前希望读者们对bert的相关理论有一定的了解，尤其是transformer的结构原理，网上的资料很多，本文内容对原理部分就不做过多的介绍了。
我自己写出来其中一个目的也是帮助自己学习整理、当你输出的时候才也会明白哪里懂了哪里不懂。因为水平有限，很多地方理解不到位的，还请各位批评指正。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="BERT系列（二）——源码解读之模型主体"/>
  <meta property="og:site_name" content="西溪雷神"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="西溪雷神" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">西溪雷神</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article id="post-BERT系列（二）——源码解读之模型主体" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-12-19T08:08:07.000Z"><a href="/BERT系列（二）——源码解读之模型主体/">2018-12-19</a></time>
      
      
  
    <h1 class="p-name title" itemprop="headline name">BERT系列（二）——源码解读之模型主体</h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>本篇文章主要是解读模型主体代码<a href="https://github.com/google-research/bert/blob/master/modeling.py" target="_blank" rel="noopener">modeling.py</a>。在阅读这篇文章之前希望读者们对bert的相关理论有一定的了解，尤其是transformer的结构原理，网上的资料很多，本文内容对原理部分就不做过多的介绍了。</p>
<p>我自己写出来其中一个目的也是帮助自己学习整理、当你输出的时候才也会明白哪里懂了哪里不懂。因为水平有限，很多地方理解不到位的，还请各位批评指正。</p>
<a id="more"></a>

<h1 id="1、配置"><a href="#1、配置" class="headerlink" title="1、配置"></a>1、配置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class BertConfig(object):</span><br><span class="line">  &quot;&quot;&quot;Configuration for &#96;BertModel&#96;.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               vocab_size,</span><br><span class="line">               hidden_size&#x3D;768,</span><br><span class="line">               num_hidden_layers&#x3D;12,</span><br><span class="line">               num_attention_heads&#x3D;12,</span><br><span class="line">               intermediate_size&#x3D;3072,</span><br><span class="line">               hidden_act&#x3D;&quot;gelu&quot;,</span><br><span class="line">               hidden_dropout_prob&#x3D;0.1,</span><br><span class="line">               attention_probs_dropout_prob&#x3D;0.1,</span><br><span class="line">               max_position_embeddings&#x3D;512,</span><br><span class="line">               type_vocab_size&#x3D;16,</span><br><span class="line">               initializer_range&#x3D;0.02):</span><br><span class="line">    self.vocab_size &#x3D; vocab_size</span><br><span class="line">    self.hidden_size &#x3D; hidden_size</span><br><span class="line">    self.num_hidden_layers &#x3D; num_hidden_layers</span><br><span class="line">    self.num_attention_heads &#x3D; num_attention_heads</span><br><span class="line">    self.hidden_act &#x3D; hidden_act</span><br><span class="line">    self.intermediate_size &#x3D; intermediate_size</span><br><span class="line">    self.hidden_dropout_prob &#x3D; hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob &#x3D; attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings &#x3D; max_position_embeddings</span><br><span class="line">    self.type_vocab_size &#x3D; type_vocab_size</span><br><span class="line">    self.initializer_range &#x3D; initializer_range</span><br></pre></td></tr></table></figure>
<p>模型配置，比较简单，依次是：词典大小、隐层神经元个数、transformer的层数、attention的头数、激活函数、中间层神经元个数、隐层dropout比例、attention里面dropout比例、sequence最大长度、token_type_ids的词典大小、truncated_normal_initializer的stdev。</p>
<h1 id="2、word-embedding"><a href="#2、word-embedding" class="headerlink" title="2、word embedding"></a>2、word embedding</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def embedding_lookup(input_ids,</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     embedding_size&#x3D;128,</span><br><span class="line">                     initializer_range&#x3D;0.02,</span><br><span class="line">                     word_embedding_name&#x3D;&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings&#x3D;False):</span><br><span class="line">  if input_ids.shape.ndims &#x3D;&#x3D; 2:</span><br><span class="line">    input_ids &#x3D; tf.expand_dims(input_ids, axis&#x3D;[-1])</span><br><span class="line"></span><br><span class="line">  embedding_table &#x3D; tf.get_variable(</span><br><span class="line">      name&#x3D;word_embedding_name,</span><br><span class="line">      shape&#x3D;[vocab_size, embedding_size],</span><br><span class="line">      initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    flat_input_ids &#x3D; tf.reshape(input_ids, [-1])</span><br><span class="line">    one_hot_input_ids &#x3D; tf.one_hot(flat_input_ids, depth&#x3D;vocab_size)</span><br><span class="line">    output &#x3D; tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output &#x3D; tf.nn.embedding_lookup(embedding_table, input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape &#x3D; get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output &#x3D; tf.reshape(output,</span><br><span class="line">                      input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure>
<p>构造embedding_table，进行word embedding，可选one_hot的方式，返回embedding的结果和embedding_table</p>
<h1 id="3、词向量的后续处理"><a href="#3、词向量的后续处理" class="headerlink" title="3、词向量的后续处理"></a>3、词向量的后续处理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type&#x3D;False,</span><br><span class="line">                            token_type_ids&#x3D;None,</span><br><span class="line">                            token_type_vocab_size&#x3D;16,</span><br><span class="line">                            token_type_embedding_name&#x3D;&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings&#x3D;True,</span><br><span class="line">                            position_embedding_name&#x3D;&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range&#x3D;0.02,</span><br><span class="line">                            max_position_embeddings&#x3D;512,</span><br><span class="line">                            dropout_prob&#x3D;0.1):</span><br><span class="line">  input_shape &#x3D; get_shape_list(input_tensor, expected_rank&#x3D;3)</span><br><span class="line">  batch_size &#x3D; input_shape[0]</span><br><span class="line">  seq_length &#x3D; input_shape[1]</span><br><span class="line">  width &#x3D; input_shape[2]</span><br><span class="line">  output &#x3D; input_tensor</span><br><span class="line">  if use_token_type:</span><br><span class="line">    if token_type_ids is None:</span><br><span class="line">      raise ValueError(&quot;&#96;token_type_ids&#96; must be specified if&quot;</span><br><span class="line">                       &quot;&#96;use_token_type&#96; is True.&quot;)</span><br><span class="line">    token_type_table &#x3D; tf.get_variable(</span><br><span class="line">        name&#x3D;token_type_embedding_name,</span><br><span class="line">        shape&#x3D;[token_type_vocab_size, width],</span><br><span class="line">        initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line">    flat_token_type_ids &#x3D; tf.reshape(token_type_ids, [-1])</span><br><span class="line">    one_hot_ids &#x3D; tf.one_hot(flat_token_type_ids, depth&#x3D;token_type_vocab_size)</span><br><span class="line">    token_type_embeddings &#x3D; tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings &#x3D; tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output +&#x3D; token_type_embeddings</span><br><span class="line">  if use_position_embeddings:</span><br><span class="line">    assert_op &#x3D; tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    with tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;position_embedding_name,</span><br><span class="line">          shape&#x3D;[max_position_embeddings, width],</span><br><span class="line">          initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line">      position_embeddings &#x3D; tf.slice(full_position_embeddings, [0, 0],</span><br><span class="line">                                     [seq_length, -1])</span><br><span class="line">      num_dims &#x3D; len(output.shape.as_list())</span><br><span class="line">      position_broadcast_shape &#x3D; []</span><br><span class="line">      for _ in range(num_dims - 2):</span><br><span class="line">        position_broadcast_shape.append(1)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings &#x3D; tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output +&#x3D; position_embeddings</span><br><span class="line">  output &#x3D; layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  return output</span><br></pre></td></tr></table></figure>
<p>主要是信息添加，可以将word的位置和word对应的token type等信息添加到词向量里面，并且layer正则化和dropout之后返回</p>
<h1 id="4、构造attention-mask"><a href="#4、构造attention-mask" class="headerlink" title="4、构造attention mask"></a>4、构造attention mask</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def create_attention_mask_from_input_mask(from_tensor, to_mask):</span><br><span class="line">  from_shape &#x3D; get_shape_list(from_tensor, expected_rank&#x3D;[2, 3])</span><br><span class="line">  batch_size &#x3D; from_shape[0]</span><br><span class="line">  from_seq_length &#x3D; from_shape[1]</span><br><span class="line">  to_shape &#x3D; get_shape_list(to_mask, expected_rank&#x3D;2)</span><br><span class="line">  to_seq_length &#x3D; to_shape[1]</span><br><span class="line">  to_mask &#x3D; tf.cast(</span><br><span class="line">      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)</span><br><span class="line">  broadcast_ones &#x3D; tf.ones(</span><br><span class="line">      shape&#x3D;[batch_size, from_seq_length, 1], dtype&#x3D;tf.float32)</span><br><span class="line">  mask &#x3D; broadcast_ones * to_mask</span><br><span class="line">  return mask</span><br></pre></td></tr></table></figure>
<p>将shape为[batch_size, to_seq_length]的2D mask转换为一个shape 为[batch_size, from_seq_length, to_seq_length] 的3D mask用于attention当中。</p>
<h1 id="5、attention-layer"><a href="#5、attention-layer" class="headerlink" title="5、attention layer"></a>5、attention layer</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask&#x3D;None,</span><br><span class="line">                    num_attention_heads&#x3D;1,</span><br><span class="line">                    size_per_head&#x3D;512,</span><br><span class="line">                    query_act&#x3D;None,</span><br><span class="line">                    key_act&#x3D;None,</span><br><span class="line">                    value_act&#x3D;None,</span><br><span class="line">                    attention_probs_dropout_prob&#x3D;0.0,</span><br><span class="line">                    initializer_range&#x3D;0.02,</span><br><span class="line">                    do_return_2d_tensor&#x3D;False,</span><br><span class="line">                    batch_size&#x3D;None,</span><br><span class="line">                    from_seq_length&#x3D;None,</span><br><span class="line">                    to_seq_length&#x3D;None):</span><br><span class="line">  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,</span><br><span class="line">                           seq_length, width):</span><br><span class="line">    output_tensor &#x3D; tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor &#x3D; tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">    return output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape &#x3D; get_shape_list(from_tensor, expected_rank&#x3D;[2, 3])</span><br><span class="line">  to_shape &#x3D; get_shape_list(to_tensor, expected_rank&#x3D;[2, 3])</span><br><span class="line"></span><br><span class="line">  if len(from_shape) !&#x3D; len(to_shape):</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The rank of &#96;from_tensor&#96; must match the rank of &#96;to_tensor&#96;.&quot;)</span><br><span class="line"></span><br><span class="line">  if len(from_shape) &#x3D;&#x3D; 3:</span><br><span class="line">    batch_size &#x3D; from_shape[0]</span><br><span class="line">    from_seq_length &#x3D; from_shape[1]</span><br><span class="line">    to_seq_length &#x3D; to_shape[1]</span><br><span class="line">  elif len(from_shape) &#x3D;&#x3D; 2:</span><br><span class="line">    if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">      raise ValueError(</span><br><span class="line">          &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">          &quot;for &#96;batch_size&#96;, &#96;from_seq_length&#96;, and &#96;to_seq_length&#96; &quot;</span><br><span class="line">          &quot;must all be specified.&quot;)</span><br><span class="line"></span><br><span class="line">  # Scalar dimensions referenced here:</span><br><span class="line">  #   B &#x3D; batch size (number of sequences)</span><br><span class="line">  #   F &#x3D; &#96;from_tensor&#96; sequence length</span><br><span class="line">  #   T &#x3D; &#96;to_tensor&#96; sequence length</span><br><span class="line">  #   N &#x3D; &#96;num_attention_heads&#96;</span><br><span class="line">  #   H &#x3D; &#96;size_per_head&#96;</span><br><span class="line"></span><br><span class="line">  from_tensor_2d &#x3D; reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d &#x3D; reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  # &#96;query_layer&#96; &#x3D; [B*F, N*H]</span><br><span class="line">  query_layer &#x3D; tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation&#x3D;query_act,</span><br><span class="line">      name&#x3D;&quot;query&quot;,</span><br><span class="line">      kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # &#96;key_layer&#96; &#x3D; [B*T, N*H]</span><br><span class="line">  key_layer &#x3D; tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation&#x3D;key_act,</span><br><span class="line">      name&#x3D;&quot;key&quot;,</span><br><span class="line">      kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # &#96;value_layer&#96; &#x3D; [B*T, N*H]</span><br><span class="line">  value_layer &#x3D; tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation&#x3D;value_act,</span><br><span class="line">      name&#x3D;&quot;value&quot;,</span><br><span class="line">      kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # &#96;query_layer&#96; &#x3D; [B, N, F, H]</span><br><span class="line">  query_layer &#x3D; transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  # &#96;key_layer&#96; &#x3D; [B, N, T, H]</span><br><span class="line">  key_layer &#x3D; transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  attention_scores &#x3D; tf.matmul(query_layer, key_layer, transpose_b&#x3D;True)</span><br><span class="line">  attention_scores &#x3D; tf.multiply(attention_scores,</span><br><span class="line">                                 1.0 &#x2F; math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  if attention_mask is not None:</span><br><span class="line">    # &#96;attention_mask&#96; &#x3D; [B, 1, F, T]</span><br><span class="line">    attention_mask &#x3D; tf.expand_dims(attention_mask, axis&#x3D;[1])</span><br><span class="line"></span><br><span class="line">    adder &#x3D; (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line"></span><br><span class="line">    attention_scores +&#x3D; adder</span><br><span class="line"></span><br><span class="line">  attention_probs &#x3D; tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  attention_probs &#x3D; dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  # &#96;value_layer&#96; &#x3D; [B, T, N, H]</span><br><span class="line">  value_layer &#x3D; tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  # &#96;value_layer&#96; &#x3D; [B, N, T, H]</span><br><span class="line">  value_layer &#x3D; tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  # &#96;context_layer&#96; &#x3D; [B, N, F, H]</span><br><span class="line">  context_layer &#x3D; tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  # &#96;context_layer&#96; &#x3D; [B, F, N, H]</span><br><span class="line">  context_layer &#x3D; tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  if do_return_2d_tensor:</span><br><span class="line">    # &#96;context_layer&#96; &#x3D; [B*F, N*V]</span><br><span class="line">    context_layer &#x3D; tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  else:</span><br><span class="line">    # &#96;context_layer&#96; &#x3D; [B, F, N*V]</span><br><span class="line">    context_layer &#x3D; tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  return context_layer</span><br></pre></td></tr></table></figure>
<p>整个网络的重头戏来了！tansformer的主要内容都在这里面，输入的from_tensor当作query，to_tensor当作key和value。当self attention的时候from_tensor和to_tensor是同一个值。</p>
<p>（1）函数一开始对输入的shape进行校验，获取batch_size、from_seq_length 、to_seq_length 。输入如果是3D张量则转化成2D矩阵(以输入为word_embedding为例[batch_size, seq_lenth, hidden_size] -&gt; [batch_size*seq_lenth, hidden_size])</p>
<p>（2）通过全连接线性投影生成query_layer、key_layer 、value_layer，输出的第二个维度变成num_attention_heads * size_per_head（整个模型默认hidden_size=num_attention_heads * size_per_head）。然后通过transpose_for_scores转换成多头。</p>
<p>（3）根据公式计算attention_probs（attention score）：<br><img src="https://upload-images.jianshu.io/upload_images/14927967-d15838fded60c359.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Attention Score计算公式"><br>如果attention_mask is not None，对mask的部分加上一个很大的负数，这样softmax之后相应的概率值接近为0，再dropout。</p>
<p>（4）最后再将value和attention_probs相乘，返回3D张量或者2D矩阵</p>
<p><strong>总结：</strong></p>
<p>同学们可以将这段代码与网络结构图对照起来看：<br><img src="https://upload-images.jianshu.io/upload_images/14927967-2056c387bd0da3c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Attention Layer"><br>该函数相比其他版本的的<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">transformer</a>很多地方都有简化，有以下四点：</p>
<p>（1）缺少scale的操作；</p>
<p>（2）没有Causality mask，个人猜测主要是bert没有decoder的操作，所以对角矩阵mask是不需要的，从另一方面来说正好体现了<strong>双向transformer</strong>的特点；</p>
<p>（3）没有query mask。跟（2）理由类似，encoder都是self attention，query和key相同所以只需要一次key mask就够了</p>
<p>（4）没有query的Residual层和normalize</p>
<h1 id="6、Transformer"><a href="#6、Transformer" class="headerlink" title="6、Transformer"></a>6、Transformer</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask&#x3D;None,</span><br><span class="line">                      hidden_size&#x3D;768,</span><br><span class="line">                      num_hidden_layers&#x3D;12,</span><br><span class="line">                      num_attention_heads&#x3D;12,</span><br><span class="line">                      intermediate_size&#x3D;3072,</span><br><span class="line">                      intermediate_act_fn&#x3D;gelu,</span><br><span class="line">                      hidden_dropout_prob&#x3D;0.1,</span><br><span class="line">                      attention_probs_dropout_prob&#x3D;0.1,</span><br><span class="line">                      initializer_range&#x3D;0.02,</span><br><span class="line">                      do_return_all_layers&#x3D;False):</span><br><span class="line">  if hidden_size % num_attention_heads !&#x3D; 0:</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">        &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size &#x3D; int(hidden_size &#x2F; num_attention_heads)</span><br><span class="line">  input_shape &#x3D; get_shape_list(input_tensor, expected_rank&#x3D;3)</span><br><span class="line">  batch_size &#x3D; input_shape[0]</span><br><span class="line">  seq_length &#x3D; input_shape[1]</span><br><span class="line">  input_width &#x3D; input_shape[2]</span><br><span class="line"></span><br><span class="line">  if input_width !&#x3D; hidden_size:</span><br><span class="line">    raise ValueError(&quot;The width of the input tensor (%d) !&#x3D; hidden size (%d)&quot; %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  prev_output &#x3D; reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs &#x3D; []</span><br><span class="line">  for layer_idx in range(num_hidden_layers):</span><br><span class="line">    with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">      layer_input &#x3D; prev_output</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">        attention_heads &#x3D; []</span><br><span class="line">        with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">          attention_head &#x3D; attention_layer(</span><br><span class="line">              from_tensor&#x3D;layer_input,</span><br><span class="line">              to_tensor&#x3D;layer_input,</span><br><span class="line">              attention_mask&#x3D;attention_mask,</span><br><span class="line">              num_attention_heads&#x3D;num_attention_heads,</span><br><span class="line">              size_per_head&#x3D;attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob&#x3D;attention_probs_dropout_prob,</span><br><span class="line">              initializer_range&#x3D;initializer_range,</span><br><span class="line">              do_return_2d_tensor&#x3D;True,</span><br><span class="line">              batch_size&#x3D;batch_size,</span><br><span class="line">              from_seq_length&#x3D;seq_length,</span><br><span class="line">              to_seq_length&#x3D;seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output &#x3D; None</span><br><span class="line">        if len(attention_heads) &#x3D;&#x3D; 1:</span><br><span class="line">          attention_output &#x3D; attention_heads[0]</span><br><span class="line">        else:</span><br><span class="line">          attention_output &#x3D; tf.concat(attention_heads, axis&#x3D;-1)</span><br><span class="line">        with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">          attention_output &#x3D; tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line">          attention_output &#x3D; dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output &#x3D; layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;intermediate&quot;):</span><br><span class="line">        intermediate_output &#x3D; tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation&#x3D;intermediate_act_fn,</span><br><span class="line">            kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">        layer_output &#x3D; tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer&#x3D;create_initializer(initializer_range))</span><br><span class="line">        layer_output &#x3D; dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output &#x3D; layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output &#x3D; layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  if do_return_all_layers:</span><br><span class="line">    final_outputs &#x3D; []</span><br><span class="line">    for layer_output in all_layer_outputs:</span><br><span class="line">      final_output &#x3D; reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    return final_outputs</span><br><span class="line">  else:</span><br><span class="line">    final_output &#x3D; reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    return final_output</span><br></pre></td></tr></table></figure>
<p>transformer是对attention的利用，分以下几步：</p>
<p>（1）计算attention_head_size，attention_head_size = int(hidden_size / num_attention_heads)即将隐层的输出等分给各个attention头。然后将input_tensor转换成2D矩阵；</p>
<p>（2）对input_tensor进行多头attention操作，再做：线性投影——dropout——layer norm——intermediate线性投影——线性投影——dropout——attention_output的residual——layer norm</p>
<p>其中intermediate线性投影的hidden_size可以自行指定，其他层的线性投影hidden_size需要统一，目的是为了对齐。</p>
<p>（3）如此循环计算若干次，且保存每一次的输出，最后返回所有层的输出或者最后一层的输出。</p>
<p><strong>总结：</strong></p>
<p>进一步证实该函数transformer只存在encoder，而不存在decoder操作，所以所有层的多头attention操作都是基于self encoder的。对应论文红框的部分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14927967-20939b0378e75097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="The Transformer - model architecture"></p>
<h1 id="7、BertModel"><a href="#7、BertModel" class="headerlink" title="7、BertModel"></a>7、BertModel</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">class BertModel(object):</span><br><span class="line">  def __init__(self,</span><br><span class="line">               config,</span><br><span class="line">               is_training,</span><br><span class="line">               input_ids,</span><br><span class="line">               input_mask&#x3D;None,</span><br><span class="line">               token_type_ids&#x3D;None,</span><br><span class="line">               use_one_hot_embeddings&#x3D;True,</span><br><span class="line">               scope&#x3D;None):</span><br><span class="line">    config &#x3D; copy.deepcopy(config)</span><br><span class="line">    if not is_training:</span><br><span class="line">      config.hidden_dropout_prob &#x3D; 0.0</span><br><span class="line">      config.attention_probs_dropout_prob &#x3D; 0.0</span><br><span class="line"></span><br><span class="line">    input_shape &#x3D; get_shape_list(input_ids, expected_rank&#x3D;2)</span><br><span class="line">    batch_size &#x3D; input_shape[0]</span><br><span class="line">    seq_length &#x3D; input_shape[1]</span><br><span class="line"></span><br><span class="line">    if input_mask is None:</span><br><span class="line">      input_mask &#x3D; tf.ones(shape&#x3D;[batch_size, seq_length], dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line">    if token_type_ids is None:</span><br><span class="line">      token_type_ids &#x3D; tf.zeros(shape&#x3D;[batch_size, seq_length], dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line">    with tf.variable_scope(scope, default_name&#x3D;&quot;bert&quot;):</span><br><span class="line">      with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">        (self.embedding_output, self.embedding_table) &#x3D; embedding_lookup(</span><br><span class="line">            input_ids&#x3D;input_ids,</span><br><span class="line">            vocab_size&#x3D;config.vocab_size,</span><br><span class="line">            embedding_size&#x3D;config.hidden_size,</span><br><span class="line">            initializer_range&#x3D;config.initializer_range,</span><br><span class="line">            word_embedding_name&#x3D;&quot;word_embeddings&quot;,</span><br><span class="line">            use_one_hot_embeddings&#x3D;use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        self.embedding_output &#x3D; embedding_postprocessor(</span><br><span class="line">            input_tensor&#x3D;self.embedding_output,</span><br><span class="line">            use_token_type&#x3D;True,</span><br><span class="line">            token_type_ids&#x3D;token_type_ids,</span><br><span class="line">            token_type_vocab_size&#x3D;config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name&#x3D;&quot;token_type_embeddings&quot;,</span><br><span class="line">            use_position_embeddings&#x3D;True,</span><br><span class="line">            position_embedding_name&#x3D;&quot;position_embeddings&quot;,</span><br><span class="line">            initializer_range&#x3D;config.initializer_range,</span><br><span class="line">            max_position_embeddings&#x3D;config.max_position_embeddings,</span><br><span class="line">            dropout_prob&#x3D;config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">        attention_mask &#x3D; create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        self.all_encoder_layers &#x3D; transformer_model(</span><br><span class="line">            input_tensor&#x3D;self.embedding_output,</span><br><span class="line">            attention_mask&#x3D;attention_mask,</span><br><span class="line">            hidden_size&#x3D;config.hidden_size,</span><br><span class="line">            num_hidden_layers&#x3D;config.num_hidden_layers,</span><br><span class="line">            num_attention_heads&#x3D;config.num_attention_heads,</span><br><span class="line">            intermediate_size&#x3D;config.intermediate_size,</span><br><span class="line">            intermediate_act_fn&#x3D;get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob&#x3D;config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob&#x3D;config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range&#x3D;config.initializer_range,</span><br><span class="line">            do_return_all_layers&#x3D;True)</span><br><span class="line"></span><br><span class="line">      self.sequence_output &#x3D; self.all_encoder_layers[-1]</span><br><span class="line">      with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">        first_token_tensor &#x3D; tf.squeeze(self.sequence_output[:, 0:1, :], axis&#x3D;1)</span><br><span class="line">        self.pooled_output &#x3D; tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation&#x3D;tf.tanh,</span><br><span class="line">            kernel_initializer&#x3D;create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure>
<p>终于到模型入口了。</p>
<p>(1）设置各种参数，如果input_mask为None的话，就指定所有input_mask值为1，即不进行过滤；如果token_type_ids是None的话，就指定所有token_type_ids值为0；</p>
<p>(2）对输入的input_ids进行embedding操作，再embedding_postprocessor操作，前面我们说了。主要是加入位置和token_type信息到词向量里面；</p>
<p>(3）转换attention_mask 后，通过调用transformer_model进行encoder操作；</p>
<p>(4）获取最后一层的输出sequence_output和pooled_output，pooled_output是取sequence_output的第一个切片然后线性投影获得（可以用于分类问题）</p>
<h1 id="8、总结："><a href="#8、总结：" class="headerlink" title="8、总结："></a>8、总结：</h1><p>（1）bert主要流程是先embedding（包括位置和token_type的embedding），然后调用transformer得到输出结果，其中embedding、embedding_table、所有transformer层输出、最后transformer层输出以及pooled_output都可以获得，用于迁移学习的fine-tune和预测任务；</p>
<p>（2）bert对于transformer的使用仅限于encoder，没有decoder的过程。这是因为模型存粹是为了预训练服务，而预训练是通过语言模型，不同于NLP其他特定任务。在做迁移学习时可以自行添加；</p>
<p>（3）正因为没有decoder的操作，所以在attention函数里面也相应地减少了很多不必要的功能。</p>
<p>其他非主要函数这里不做过多介绍，感兴趣的同学可以去看源码。</p>
<p>下一篇文章我们将继续学习bert源码的其他模块，包括训练、预测以及输入输出等相关功能。</p>
<p><strong>本文上一篇系列</strong></p>
<p><a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a><br><a href="../BERT系列（三）——源码解读之Pre-train">BERT系列（三）——源码解读之Pre-train</a><br><a href="../BERT系列（四）——源码解读之Fine-tune">BERT系列（四）——源码解读之Fine-tune</a><br><a href="../BERT系列（五）——中文分词实践-F1-97-8-附代码">BERT系列（五）——中文分词实践 F1 97.8%(附代码)</a></p>
<p><strong>Reference</strong></p>
<p>1.<a href="https://github.com/google-research/bert/blob/master/modeling.py" target="_blank" rel="noopener">https://github.com/google-research/bert/blob/master/modeling.py</a><br>2.<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">https://github.com/Kyubyong/transformer</a><br>3.<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a><br>4.<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding</a></p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="https://jiangpinglei.github.io/BERT%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8B%E6%A8%A1%E5%9E%8B%E4%B8%BB%E4%BD%93/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="jiangpinglei.github.io">
  </form>
</div>


  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Jiangping Lei
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/js/jquery-3.4.1.min.js"></script>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
