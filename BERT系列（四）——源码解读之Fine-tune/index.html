<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>BERT系列（四）——源码解读之Fine-tune | 西溪雷神</title>
  <meta name="author" content="Jiangping Lei">
  
  <meta name="description" content="这是我们源码解读的最后一个部分了。fine-tune搞明白之后推断也就没必要再分析了，反正形式都是一样的，重要的是明白根据不同任务调整输入格式和对loss的构建，这两个知识点学会之后，基本上也可以依葫芦画瓢做一些自己的任务了。
bert官方给了两个任务的fine-tune代码:
1.run_classifier.py
2.run_squad.py
其实就是我们在BERT系列（一）——demo运行里运行的demo，下面我就对这两个代码进行展开说明：">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="BERT系列（四）——源码解读之Fine-tune"/>
  <meta property="og:site_name" content="西溪雷神"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="西溪雷神" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">西溪雷神</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article id="post-BERT系列（四）——源码解读之Fine-tune" class="h-entry post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time class="dt-published" datetime="2018-12-26T10:30:57.000Z"><a href="/BERT系列（四）——源码解读之Fine-tune/">2018-12-26</a></time>
      
      
  
    <h1 class="p-name title" itemprop="headline name">BERT系列（四）——源码解读之Fine-tune</h1>
  

    </header>
    <div class="e-content entry" itemprop="articleBody">
      
        <p>这是我们源码解读的最后一个部分了。fine-tune搞明白之后推断也就没必要再分析了，反正形式都是一样的，重要的是明白根据不同任务调整输入格式和对loss的构建，这两个知识点学会之后，基本上也可以依葫芦画瓢做一些自己的任务了。</p>
<p>bert官方给了两个任务的fine-tune代码:</p>
<p>1.<a href="https://github.com/google-research/bert/blob/master/run_classifier.py" target="_blank" rel="noopener">run_classifier.py</a></p>
<p>2.<a href="https://github.com/google-research/bert/blob/master/run_squad.py" target="_blank" rel="noopener">run_squad.py</a></p>
<p>其实就是我们在<a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a>里运行的demo，下面我就对这两个代码进行展开说明：</p>
<a id="more"></a>

<h1 id="一、run-classifier-py"><a href="#一、run-classifier-py" class="headerlink" title="一、run_classifier.py"></a>一、run_classifier.py</h1><h2 id="1、参数"><a href="#1、参数" class="headerlink" title="1、参数"></a>1、参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">## Required parameters</span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    &quot;data_dir&quot;, None,</span><br><span class="line">    &quot;The input data dir. Should contain the .tsv files (or other data files) &quot;</span><br><span class="line">    &quot;for the task.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    &quot;bert_config_file&quot;, None,</span><br><span class="line">    &quot;The config json file corresponding to the pre-trained BERT model. &quot;</span><br><span class="line">    &quot;This specifies the model architecture.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(&quot;task_name&quot;, None, &quot;The name of the task to train.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(&quot;vocab_file&quot;, None,</span><br><span class="line">                    &quot;The vocabulary file that the BERT model was trained on.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    &quot;output_dir&quot;, None,</span><br><span class="line">    &quot;The output directory where the model checkpoints will be written.&quot;)</span><br><span class="line"></span><br><span class="line">## Other parameters</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    &quot;init_checkpoint&quot;, None,</span><br><span class="line">    &quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    &quot;do_lower_case&quot;, True,</span><br><span class="line">    &quot;Whether to lower case the input text. Should be True for uncased &quot;</span><br><span class="line">    &quot;models and False for cased models.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    &quot;max_seq_length&quot;, 128,</span><br><span class="line">    &quot;The maximum total input sequence length after WordPiece tokenization. &quot;</span><br><span class="line">    &quot;Sequences longer than this will be truncated, and sequences shorter &quot;</span><br><span class="line">    &quot;than this will be padded.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(&quot;do_train&quot;, False, &quot;Whether to run training.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(&quot;do_eval&quot;, False, &quot;Whether to run eval on the dev set.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    &quot;do_predict&quot;, False,</span><br><span class="line">    &quot;Whether to run the model in inference mode on the test set.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(&quot;train_batch_size&quot;, 32, &quot;Total batch size for training.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(&quot;eval_batch_size&quot;, 8, &quot;Total batch size for eval.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(&quot;predict_batch_size&quot;, 8, &quot;Total batch size for predict.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(&quot;learning_rate&quot;, 5e-5, &quot;The initial learning rate for Adam.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(&quot;num_train_epochs&quot;, 3.0,</span><br><span class="line">                   &quot;Total number of training epochs to perform.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(</span><br><span class="line">    &quot;warmup_proportion&quot;, 0.1,</span><br><span class="line">    &quot;Proportion of training to perform linear learning rate warmup for. &quot;</span><br><span class="line">    &quot;E.g., 0.1 &#x3D; 10% of training.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(&quot;save_checkpoints_steps&quot;, 1000,</span><br><span class="line">                     &quot;How often to save the model checkpoint.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(&quot;iterations_per_loop&quot;, 1000,</span><br><span class="line">                     &quot;How many steps to make in each estimator call.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(&quot;use_tpu&quot;, False, &quot;Whether to use TPU or GPU&#x2F;CPU.&quot;)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    &quot;tpu_name&quot;, None,</span><br><span class="line">    &quot;The Cloud TPU to use for training. This should be either the name &quot;</span><br><span class="line">    &quot;used when creating the Cloud TPU, or a grpc:&#x2F;&#x2F;ip.address.of.tpu:8470 &quot;</span><br><span class="line">    &quot;url.&quot;)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    &quot;tpu_zone&quot;, None,</span><br><span class="line">    &quot;[Optional] GCE zone where the Cloud TPU is located in. If not &quot;</span><br><span class="line">    &quot;specified, we will attempt to automatically detect the GCE project from &quot;</span><br><span class="line">    &quot;metadata.&quot;)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    &quot;gcp_project&quot;, None,</span><br><span class="line">    &quot;[Optional] Project name for the Cloud TPU-enabled project. If not &quot;</span><br><span class="line">    &quot;specified, we will attempt to automatically detect the GCE project from &quot;</span><br><span class="line">    &quot;metadata.&quot;)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(&quot;master&quot;, None, &quot;[Optional] TensorFlow master URL.&quot;)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    &quot;num_tpu_cores&quot;, 8,</span><br><span class="line">    &quot;Only used if &#96;use_tpu&#96; is True. Total number of TPU cores to use.&quot;)</span><br></pre></td></tr></table></figure>
<p>这些参数相信运行过demo的同学都已经认识了，不认识读读上面的英文解释也大概能明白什么意思。其中有两个可能需要说明下：</p>
<p>max_seq_length：指定WordPiece tokenization 之后的sequence的最大长度，要求小于等于预训练模型的最大sequence长度。当输入的数据长度小于max_seq_length时用0补齐，如果长度大于max_seq_length则truncate处理；</p>
<p>warmup_proportion：warm up 步数的比例，比如说总共学习100步，warmup_proportion=0.1表示前10步用来warm up，warm up时以较低的学习率进行学习(lr = global_step/num_warmup_steps * init_lr)，10步之后以正常(或衰减)的学习率来学习。至于这么做的目的不太明白，有知道的同学请务必留言告诉我下，感激不尽。</p>
<h2 id="2、数据预处理（以MRPC为例）"><a href="#2、数据预处理（以MRPC为例）" class="headerlink" title="2、数据预处理（以MRPC为例）"></a>2、数据预处理（以MRPC为例）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class InputExample(object):</span><br><span class="line">  &quot;&quot;&quot;A single training&#x2F;test example for simple sequence classification.&quot;&quot;&quot;</span><br><span class="line">  def __init__(self, guid, text_a, text_b&#x3D;None, label&#x3D;None):</span><br><span class="line">    self.guid &#x3D; guid</span><br><span class="line">    self.text_a &#x3D; text_a</span><br><span class="line">    self.text_b &#x3D; text_b</span><br><span class="line">    self.label &#x3D; label</span><br></pre></td></tr></table></figure>
<p>这是输入语料样本的数据结构。</p>
<p>guid是该样本的唯一ID，text_a和text_b表示句子对，lable表示句子对关系，如果是test数据集则label统一为0。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class InputFeatures(object):</span><br><span class="line">  &quot;&quot;&quot;A single set of features of data.&quot;&quot;&quot;</span><br><span class="line">  def __init__(self, input_ids, input_mask, segment_ids, label_id):</span><br><span class="line">    self.input_ids &#x3D; input_ids</span><br><span class="line">    self.input_mask &#x3D; input_mask</span><br><span class="line">    self.segment_ids &#x3D; segment_ids</span><br><span class="line">    self.label_id &#x3D; label_id</span><br></pre></td></tr></table></figure>
<p>tokenization过后的样本数据结构，input_ids其实就是tokens的索引，input_mask不用解释，segment_ids对应模型的token_type_ids以上三者构成模型输入的X，label_id是标签，对应Y。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class MrpcProcessor(DataProcessor):</span><br><span class="line">  &quot;&quot;&quot;Processor for the MRPC data set (GLUE version).&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def get_train_examples(self, data_dir):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    return self._create_examples(</span><br><span class="line">        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)</span><br><span class="line"></span><br><span class="line">  def get_dev_examples(self, data_dir):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    return self._create_examples(</span><br><span class="line">        self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)</span><br><span class="line"></span><br><span class="line">  def get_test_examples(self, data_dir):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    return self._create_examples(</span><br><span class="line">        self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;)</span><br><span class="line"></span><br><span class="line">  def get_labels(self):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    return [&quot;0&quot;, &quot;1&quot;]</span><br><span class="line"></span><br><span class="line">  def _create_examples(self, lines, set_type):</span><br><span class="line">    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;</span><br><span class="line">    examples &#x3D; []</span><br><span class="line">    for (i, line) in enumerate(lines):</span><br><span class="line">      if i &#x3D;&#x3D; 0:</span><br><span class="line">        continue</span><br><span class="line">      guid &#x3D; &quot;%s-%s&quot; % (set_type, i)</span><br><span class="line">      text_a &#x3D; tokenization.convert_to_unicode(line[3])</span><br><span class="line">      text_b &#x3D; tokenization.convert_to_unicode(line[4])</span><br><span class="line">      if set_type &#x3D;&#x3D; &quot;test&quot;:</span><br><span class="line">        label &#x3D; &quot;0&quot;</span><br><span class="line">      else:</span><br><span class="line">        label &#x3D; tokenization.convert_to_unicode(line[0])</span><br><span class="line">      examples.append(</span><br><span class="line">          InputExample(guid&#x3D;guid, text_a&#x3D;text_a, text_b&#x3D;text_b, label&#x3D;label))</span><br><span class="line">    return examples</span><br></pre></td></tr></table></figure>
<p>MRPC的数据解析器，输入格式：</p>
<blockquote><p>label 句子1ID 句子2ID 句子1 句子2</p>
</blockquote>

<p>输出的格式为InputExample数据结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def file_based_convert_examples_to_features(</span><br><span class="line">    examples, label_list, max_seq_length, tokenizer, output_file):</span><br><span class="line">  &quot;&quot;&quot;Convert a set of &#96;InputExample&#96;s to a TFRecord file.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  writer &#x3D; tf.python_io.TFRecordWriter(output_file)</span><br><span class="line"></span><br><span class="line">  for (ex_index, example) in enumerate(examples):</span><br><span class="line">    if ex_index % 10000 &#x3D;&#x3D; 0:</span><br><span class="line">      tf.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples)))</span><br><span class="line"></span><br><span class="line">    feature &#x3D; convert_single_example(ex_index, example, label_list,</span><br><span class="line">                                     max_seq_length, tokenizer)</span><br><span class="line"></span><br><span class="line">    def create_int_feature(values):</span><br><span class="line">      f &#x3D; tf.train.Feature(int64_list&#x3D;tf.train.Int64List(value&#x3D;list(values)))</span><br><span class="line">      return f</span><br><span class="line"></span><br><span class="line">    features &#x3D; collections.OrderedDict()</span><br><span class="line">    features[&quot;input_ids&quot;] &#x3D; create_int_feature(feature.input_ids)</span><br><span class="line">    features[&quot;input_mask&quot;] &#x3D; create_int_feature(feature.input_mask)</span><br><span class="line">    features[&quot;segment_ids&quot;] &#x3D; create_int_feature(feature.segment_ids)</span><br><span class="line">    features[&quot;label_ids&quot;] &#x3D; create_int_feature([feature.label_id])</span><br><span class="line"></span><br><span class="line">    tf_example &#x3D; tf.train.Example(features&#x3D;tf.train.Features(feature&#x3D;features))</span><br><span class="line">    writer.write(tf_example.SerializeToString())</span><br></pre></td></tr></table></figure>
<p>将examples转换成features，用到的函数是convert_single_example：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def convert_single_example(ex_index, example, label_list, max_seq_length,</span><br><span class="line">                           tokenizer):</span><br><span class="line">  &quot;&quot;&quot;Converts a single &#96;InputExample&#96; into a single &#96;InputFeatures&#96;.&quot;&quot;&quot;</span><br><span class="line">  label_map &#x3D; &#123;&#125;</span><br><span class="line">  for (i, label) in enumerate(label_list):</span><br><span class="line">    label_map[label] &#x3D; i</span><br><span class="line"></span><br><span class="line">  tokens_a &#x3D; tokenizer.tokenize(example.text_a)</span><br><span class="line">  tokens_b &#x3D; None</span><br><span class="line">  if example.text_b:</span><br><span class="line">    tokens_b &#x3D; tokenizer.tokenize(example.text_b)</span><br><span class="line"></span><br><span class="line">  if tokens_b:</span><br><span class="line">    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)</span><br><span class="line">  else:</span><br><span class="line">    # Account for [CLS] and [SEP] with &quot;- 2&quot;</span><br><span class="line">    if len(tokens_a) &gt; max_seq_length - 2:</span><br><span class="line">      tokens_a &#x3D; tokens_a[0:(max_seq_length - 2)]</span><br><span class="line"></span><br><span class="line">  tokens &#x3D; []</span><br><span class="line">  segment_ids &#x3D; []</span><br><span class="line">  tokens.append(&quot;[CLS]&quot;)</span><br><span class="line">  segment_ids.append(0)</span><br><span class="line">  for token in tokens_a:</span><br><span class="line">    tokens.append(token)</span><br><span class="line">    segment_ids.append(0)</span><br><span class="line">  tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">  segment_ids.append(0)</span><br><span class="line"></span><br><span class="line">  if tokens_b:</span><br><span class="line">    for token in tokens_b:</span><br><span class="line">      tokens.append(token)</span><br><span class="line">      segment_ids.append(1)</span><br><span class="line">    tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">    segment_ids.append(1)</span><br><span class="line"></span><br><span class="line">  input_ids &#x3D; tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line">  input_mask &#x3D; [1] * len(input_ids)</span><br><span class="line"></span><br><span class="line">  # Zero-pad up to the sequence length.</span><br><span class="line">  while len(input_ids) &lt; max_seq_length:</span><br><span class="line">    input_ids.append(0)</span><br><span class="line">    input_mask.append(0)</span><br><span class="line">    segment_ids.append(0)</span><br><span class="line"></span><br><span class="line">  assert len(input_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line">  assert len(input_mask) &#x3D;&#x3D; max_seq_length</span><br><span class="line">  assert len(segment_ids) &#x3D;&#x3D; max_seq_length</span><br><span class="line"></span><br><span class="line">  label_id &#x3D; label_map[example.label]</span><br><span class="line">  if ex_index &lt; 5:</span><br><span class="line">    tf.logging.info(&quot;*** Example ***&quot;)</span><br><span class="line">    tf.logging.info(&quot;guid: %s&quot; % (example.guid))</span><br><span class="line">    tf.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join(</span><br><span class="line">        [tokenization.printable_text(x) for x in tokens]))</span><br><span class="line">    tf.logging.info(&quot;input_ids: %s&quot; % &quot; &quot;.join([str(x) for x in input_ids]))</span><br><span class="line">    tf.logging.info(&quot;input_mask: %s&quot; % &quot; &quot;.join([str(x) for x in input_mask]))</span><br><span class="line">    tf.logging.info(&quot;segment_ids: %s&quot; % &quot; &quot;.join([str(x) for x in segment_ids]))</span><br><span class="line">    tf.logging.info(&quot;label: %s (id &#x3D; %d)&quot; % (example.label, label_id))</span><br><span class="line"></span><br><span class="line">  feature &#x3D; InputFeatures(</span><br><span class="line">      input_ids&#x3D;input_ids,</span><br><span class="line">      input_mask&#x3D;input_mask,</span><br><span class="line">      segment_ids&#x3D;segment_ids,</span><br><span class="line">      label_id&#x3D;label_id)</span><br><span class="line">  return feature</span><br></pre></td></tr></table></figure>
<p>把一个InputExample数据转换成InputFeatures数据结构。</p>
<p>（1）构造label_map ，因为label_list就[“0”, “1”]所以，label_map ={“0”:0, “1”:1}；</p>
<p>（2）将text_a和text_b转化成token_a和token_b，并且将二者截取到长度之和为max_seq_length - 3，如果只有token_a没有token_b，则将tokens_a截取到长度为max_seq_length - 2；</p>
<p>（3）构造tokens和segment_ids，如果不满足长度用0补齐，并且构造input_mask。</p>
<p>##3、模型构建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,</span><br><span class="line">                 labels, num_labels, use_one_hot_embeddings):</span><br><span class="line">  &quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span><br><span class="line">  model &#x3D; modeling.BertModel(</span><br><span class="line">      config&#x3D;bert_config,</span><br><span class="line">      is_training&#x3D;is_training,</span><br><span class="line">      input_ids&#x3D;input_ids,</span><br><span class="line">      input_mask&#x3D;input_mask,</span><br><span class="line">      token_type_ids&#x3D;segment_ids,</span><br><span class="line">      use_one_hot_embeddings&#x3D;use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">  output_layer &#x3D; model.get_pooled_output()</span><br><span class="line"></span><br><span class="line">  hidden_size &#x3D; output_layer.shape[-1].value</span><br><span class="line"></span><br><span class="line">  output_weights &#x3D; tf.get_variable(</span><br><span class="line">      &quot;output_weights&quot;, [num_labels, hidden_size],</span><br><span class="line">      initializer&#x3D;tf.truncated_normal_initializer(stddev&#x3D;0.02))</span><br><span class="line"></span><br><span class="line">  output_bias &#x3D; tf.get_variable(</span><br><span class="line">      &quot;output_bias&quot;, [num_labels], initializer&#x3D;tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;loss&quot;):</span><br><span class="line">    if is_training:</span><br><span class="line">      # I.e., 0.1 dropout</span><br><span class="line">      output_layer &#x3D; tf.nn.dropout(output_layer, keep_prob&#x3D;0.9)</span><br><span class="line"></span><br><span class="line">    logits &#x3D; tf.matmul(output_layer, output_weights, transpose_b&#x3D;True)</span><br><span class="line">    logits &#x3D; tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    probabilities &#x3D; tf.nn.softmax(logits, axis&#x3D;-1)</span><br><span class="line">    log_probs &#x3D; tf.nn.log_softmax(logits, axis&#x3D;-1)</span><br><span class="line"></span><br><span class="line">    one_hot_labels &#x3D; tf.one_hot(labels, depth&#x3D;num_labels, dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">    per_example_loss &#x3D; -tf.reduce_sum(one_hot_labels * log_probs, axis&#x3D;-1)</span><br><span class="line">    loss &#x3D; tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">    return (loss, per_example_loss, logits, probabilities)</span><br></pre></td></tr></table></figure>
<p>X和Y都已经构造好了，将X作为模型的输入，剩下的就是将模型输出和Y进行计算得到loss。</p>
<p>这里的模型输出取的是pooled_output，之前我们已经说过pooled_output是模型最后一层的第一个片段。之后再用一个全连接+softmax和labels的one_hot计算loss。</p>
<h1 id="二、run-squad-py"><a href="#二、run-squad-py" class="headerlink" title="二、run_squad.py"></a>二、run_squad.py</h1><p>run_squad是基于SQuAD数据进行阅读理解任务的fine-tune，除了X/Y数据的转换、loss构建其他和run_classifier是一样的，下面我们重点学习下这两块。</p>
<h2 id="1、X-Y数据的转换"><a href="#1、X-Y数据的转换" class="headerlink" title="1、X/Y数据的转换"></a>1、X/Y数据的转换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class SquadExample(object):</span><br><span class="line">  def __init__(self,</span><br><span class="line">               qas_id,</span><br><span class="line">               question_text,</span><br><span class="line">               doc_tokens,</span><br><span class="line">               orig_answer_text&#x3D;None,</span><br><span class="line">               start_position&#x3D;None,</span><br><span class="line">               end_position&#x3D;None,</span><br><span class="line">               is_impossible&#x3D;False):</span><br><span class="line">    self.qas_id &#x3D; qas_id</span><br><span class="line">    self.question_text &#x3D; question_text </span><br><span class="line">    self.doc_tokens &#x3D; doc_tokens </span><br><span class="line">    self.orig_answer_text &#x3D; orig_answer_text </span><br><span class="line">    self.start_position &#x3D; start_position</span><br><span class="line">    self.end_position &#x3D; end_position</span><br><span class="line">    self.is_impossible &#x3D; is_impossible</span><br></pre></td></tr></table></figure>
<p>qas_id 样本ID，question_text问题文本，doc_tokens阅读材料[word0, word1, …]的形式，orig_answer_text 原始答案的文本，start_position答案在文本中开始的位置，end_position答案在文本中结束的位置，is_impossible在SQuAD2里才会用到的字段这里可以不用关心。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class InputFeatures(object):</span><br><span class="line">  &quot;&quot;&quot;A single set of features of data.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               unique_id,</span><br><span class="line">               example_index,</span><br><span class="line">               doc_span_index,</span><br><span class="line">               tokens,</span><br><span class="line">               token_to_orig_map,</span><br><span class="line">               token_is_max_context,</span><br><span class="line">               input_ids,</span><br><span class="line">               input_mask,</span><br><span class="line">               segment_ids,</span><br><span class="line">               start_position&#x3D;None,</span><br><span class="line">               end_position&#x3D;None,</span><br><span class="line">               is_impossible&#x3D;None):</span><br><span class="line">    self.unique_id &#x3D; unique_id</span><br><span class="line">    self.example_index &#x3D; example_index</span><br><span class="line">    self.doc_span_index &#x3D; doc_span_index</span><br><span class="line">    self.tokens &#x3D; tokens</span><br><span class="line">    self.token_to_orig_map &#x3D; token_to_orig_map</span><br><span class="line">    self.token_is_max_context &#x3D; token_is_max_context</span><br><span class="line">    self.input_ids &#x3D; input_ids</span><br><span class="line">    self.input_mask &#x3D; input_mask</span><br><span class="line">    self.segment_ids &#x3D; segment_ids</span><br><span class="line">    self.start_position &#x3D; start_position</span><br><span class="line">    self.end_position &#x3D; end_position</span><br><span class="line">    self.is_impossible &#x3D; is_impossible</span><br></pre></td></tr></table></figure>
<p>unique_id feature的唯一id，example_index样本的索引，用于建立feature和example的对应，</p>
<p>doc_span_index该feature在doc_span的索引，如果一个文本很长，那么势必需要对其进行截取，截取成若干片段装进doc_span，doc_span里的各个片段会装进各个feature里面，所以一个feature对应的就会有一个doc_span_index；</p>
<p>tokens该样本的token序列，token_to_orig_map是tokens里面每一个token在原始doc_token的索引；</p>
<p>token_is_max_context是一个序列，里面的值表示该位置的token在当前span里面是否是最全上下文的。</p>
<p>例如bought这个词</p>
<blockquote><p>Doc: the man went to the store and bought a gallon of milk<br>Span A: the man went to the<br>Span B: to the store and bought<br>Span C: and bought a gallon of</p>
</blockquote>

<p>bought在spanB和spanC里都有出现，但很显然span C里bought是语境最全的，既有上文也有下文</p>
<p>input_ids 是tokens转化为token id作为模型的输入，input_mask 、segment_ids、is_impossible 不用多说了；</p>
<p>start_position 、 end_position为答案在当前tokens序列里面的位置（跟上面的不同，不是整个context里面的位置），需要注意的是如果答案不在当前span里的话，start_position 、 end_position均为0 。</p>
<p>SquadExample到InputFeatures转换的过程也是类似的，不用细讲，与run_classifier唯一不同的是classifier的输入是<strong>[CLS]句子a[SEP]句子b[SEP]</strong>, 而squad是<strong>[CLS]问题[SEP]阅读材料片段[SEP]</strong>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_ids &#x3D; features[&quot;input_ids&quot;]</span><br><span class="line">input_mask &#x3D; features[&quot;input_mask&quot;]</span><br><span class="line">segment_ids &#x3D; features[&quot;segment_ids&quot;]</span><br></pre></td></tr></table></figure>
<p>和这三个元素作为模型的输入X，而start_position和end_position作为Y，如果知道了Y就等于知道了答案的位置，然后反向在阅读材料context里面去找出来就可以了，逻辑大概就是这样。</p>
<h2 id="2、loss构建"><a href="#2、loss构建" class="headerlink" title="2、loss构建"></a>2、loss构建</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">def model_fn_builder(bert_config, init_checkpoint, learning_rate,</span><br><span class="line">                     num_train_steps, num_warmup_steps, use_tpu,</span><br><span class="line">                     use_one_hot_embeddings):</span><br><span class="line">  &quot;&quot;&quot;Returns &#96;model_fn&#96; closure for TPUEstimator.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def model_fn(features, labels, mode, params):  # pylint: disable&#x3D;unused-argument</span><br><span class="line">    &quot;&quot;&quot;The &#96;model_fn&#96; for TPUEstimator.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    tf.logging.info(&quot;*** Features ***&quot;)</span><br><span class="line">    for name in sorted(features.keys()):</span><br><span class="line">      tf.logging.info(&quot;  name &#x3D; %s, shape &#x3D; %s&quot; % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    unique_ids &#x3D; features[&quot;unique_ids&quot;]</span><br><span class="line">    input_ids &#x3D; features[&quot;input_ids&quot;]</span><br><span class="line">    input_mask &#x3D; features[&quot;input_mask&quot;]</span><br><span class="line">    segment_ids &#x3D; features[&quot;segment_ids&quot;]</span><br><span class="line"></span><br><span class="line">    is_training &#x3D; (mode &#x3D;&#x3D; tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    (start_logits, end_logits) &#x3D; create_model(</span><br><span class="line">        bert_config&#x3D;bert_config,</span><br><span class="line">        is_training&#x3D;is_training,</span><br><span class="line">        input_ids&#x3D;input_ids,</span><br><span class="line">        input_mask&#x3D;input_mask,</span><br><span class="line">        segment_ids&#x3D;segment_ids,</span><br><span class="line">        use_one_hot_embeddings&#x3D;use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    tvars &#x3D; tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    initialized_variable_names &#x3D; &#123;&#125;</span><br><span class="line">    scaffold_fn &#x3D; None</span><br><span class="line">    if init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) &#x3D; modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      if use_tpu:</span><br><span class="line"></span><br><span class="line">        def tpu_scaffold():</span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          return tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn &#x3D; tpu_scaffold</span><br><span class="line">      else:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(&quot;**** Trainable Variables ****&quot;)</span><br><span class="line">    for var in tvars:</span><br><span class="line">      init_string &#x3D; &quot;&quot;</span><br><span class="line">      if var.name in initialized_variable_names:</span><br><span class="line">        init_string &#x3D; &quot;, *INIT_FROM_CKPT*&quot;</span><br><span class="line">      tf.logging.info(&quot;  name &#x3D; %s, shape &#x3D; %s%s&quot;, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec &#x3D; None</span><br><span class="line">    if mode &#x3D;&#x3D; tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      seq_length &#x3D; modeling.get_shape_list(input_ids)[1]</span><br><span class="line"></span><br><span class="line">      def compute_loss(logits, positions):</span><br><span class="line">        one_hot_positions &#x3D; tf.one_hot(</span><br><span class="line">            positions, depth&#x3D;seq_length, dtype&#x3D;tf.float32)</span><br><span class="line">        log_probs &#x3D; tf.nn.log_softmax(logits, axis&#x3D;-1)</span><br><span class="line">        loss &#x3D; -tf.reduce_mean(</span><br><span class="line">            tf.reduce_sum(one_hot_positions * log_probs, axis&#x3D;-1))</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">      start_positions &#x3D; features[&quot;start_positions&quot;]</span><br><span class="line">      end_positions &#x3D; features[&quot;end_positions&quot;]</span><br><span class="line"></span><br><span class="line">      start_loss &#x3D; compute_loss(start_logits, start_positions)</span><br><span class="line">      end_loss &#x3D; compute_loss(end_logits, end_positions)</span><br><span class="line"></span><br><span class="line">      total_loss &#x3D; (start_loss + end_loss) &#x2F; 2.0</span><br><span class="line"></span><br><span class="line">      train_op &#x3D; optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec &#x3D; tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode&#x3D;mode,</span><br><span class="line">          loss&#x3D;total_loss,</span><br><span class="line">          train_op&#x3D;train_op,</span><br><span class="line">          scaffold_fn&#x3D;scaffold_fn)</span><br><span class="line">    elif mode &#x3D;&#x3D; tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">      predictions &#x3D; &#123;</span><br><span class="line">          &quot;unique_ids&quot;: unique_ids,</span><br><span class="line">          &quot;start_logits&quot;: start_logits,</span><br><span class="line">          &quot;end_logits&quot;: end_logits,</span><br><span class="line">      &#125;</span><br><span class="line">      output_spec &#x3D; tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode&#x3D;mode, predictions&#x3D;predictions, scaffold_fn&#x3D;scaffold_fn)</span><br><span class="line">    else:</span><br><span class="line">      raise ValueError(</span><br><span class="line">          &quot;Only TRAIN and PREDICT modes are supported: %s&quot; % (mode))</span><br><span class="line"></span><br><span class="line">    return output_spec</span><br><span class="line"></span><br><span class="line">  return model_fn</span><br></pre></td></tr></table></figure>
<p>从上面的代码我们可以发现，loss由两部分组成，答案start_positions的预测和end_positions的预测。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,</span><br><span class="line">                 use_one_hot_embeddings):</span><br><span class="line">  &quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;</span><br><span class="line">  model &#x3D; modeling.BertModel(</span><br><span class="line">      config&#x3D;bert_config,</span><br><span class="line">      is_training&#x3D;is_training,</span><br><span class="line">      input_ids&#x3D;input_ids,</span><br><span class="line">      input_mask&#x3D;input_mask,</span><br><span class="line">      token_type_ids&#x3D;segment_ids,</span><br><span class="line">      use_one_hot_embeddings&#x3D;use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">  final_hidden &#x3D; model.get_sequence_output()</span><br><span class="line"></span><br><span class="line">  final_hidden_shape &#x3D; modeling.get_shape_list(final_hidden, expected_rank&#x3D;3)</span><br><span class="line">  batch_size &#x3D; final_hidden_shape[0]</span><br><span class="line">  seq_length &#x3D; final_hidden_shape[1]</span><br><span class="line">  hidden_size &#x3D; final_hidden_shape[2]</span><br><span class="line"></span><br><span class="line">  output_weights &#x3D; tf.get_variable(</span><br><span class="line">      &quot;cls&#x2F;squad&#x2F;output_weights&quot;, [2, hidden_size],</span><br><span class="line">      initializer&#x3D;tf.truncated_normal_initializer(stddev&#x3D;0.02))</span><br><span class="line"></span><br><span class="line">  output_bias &#x3D; tf.get_variable(</span><br><span class="line">      &quot;cls&#x2F;squad&#x2F;output_bias&quot;, [2], initializer&#x3D;tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">  final_hidden_matrix &#x3D; tf.reshape(final_hidden,</span><br><span class="line">                                   [batch_size * seq_length, hidden_size])</span><br><span class="line">  logits &#x3D; tf.matmul(final_hidden_matrix, output_weights, transpose_b&#x3D;True)</span><br><span class="line">  logits &#x3D; tf.nn.bias_add(logits, output_bias)</span><br><span class="line"></span><br><span class="line">  logits &#x3D; tf.reshape(logits, [batch_size, seq_length, 2])</span><br><span class="line">  logits &#x3D; tf.transpose(logits, [2, 0, 1])</span><br><span class="line"></span><br><span class="line">  unstacked_logits &#x3D; tf.unstack(logits, axis&#x3D;0)</span><br><span class="line"></span><br><span class="line">  (start_logits, end_logits) &#x3D; (unstacked_logits[0], unstacked_logits[1])</span><br><span class="line"></span><br><span class="line">  return (start_logits, end_logits)</span><br></pre></td></tr></table></figure>
<p>模型的输出来自于sequence_output，即模型最后一层的输出，shape为[batch_size, seq_length, hidden_size ]，之后再加一个全连接层，unpack成两个部分，分别对应答案的两个位置。</p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>以上便是这两个demo的全部解读，squad里有很多细节特别是sample到feature的转换过程，比较复杂，但因为时间有限我们不做具体介绍，感兴趣的同学可以自己深入阅读一下。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14927967-4e7bcb20d834091f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="不同任务的输入输出示意图"><br>这两个任务可以和论文里面的示意图结合起来看，句子对分类任务对应的是图(a)，阅读理解任务对应的是图(c)</p>
<p><strong>本文系列</strong><br><a href="../BERT系列（一）——demo运行">BERT系列（一）——demo运行</a><br><a href="../BERT系列（二）——源码解读之模型主体">BERT系列（二）——模型主体源码解读</a><br><a href="../BERT系列（三）——源码解读之Pre-train">BERT系列（三）——源码解读之Pre-train</a><br><a href="../BERT系列（五）——中文分词实践-F1-97-8-附代码">BERT系列（五）——中文分词实践 F1 97.8%(附代码)</a></p>
<p><strong>Reference</strong><br>1.<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a><br>2.<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding</a></p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="https://jiangpinglei.github.io/BERT%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BFine-tune/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="as_sitesearch" value="jiangpinglei.github.io">
  </form>
</div>


  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Jiangping Lei
  
</div>
<div class="clearfix"></div></footer>
  
<script src="/js/jquery-3.4.1.min.js"></script>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
